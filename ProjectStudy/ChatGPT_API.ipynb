{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39m# Extract relevant information\u001b[39;00m\n\u001b[0;32m     23\u001b[0m results \u001b[39m=\u001b[39m []\n\u001b[1;32m---> 24\u001b[0m \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m data[\u001b[39m'\u001b[39;49m\u001b[39mitems\u001b[39;49m\u001b[39m'\u001b[39;49m]:\n\u001b[0;32m     25\u001b[0m     results\u001b[39m.\u001b[39mappend({\n\u001b[0;32m     26\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m: item[\u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     27\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mlink\u001b[39m\u001b[39m'\u001b[39m: item[\u001b[39m'\u001b[39m\u001b[39mlink\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     28\u001b[0m         \u001b[39m'\u001b[39m\u001b[39msnippet\u001b[39m\u001b[39m'\u001b[39m: item[\u001b[39m'\u001b[39m\u001b[39msnippet\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     29\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m'\u001b[39m: item\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mpagemap\u001b[39m\u001b[39m'\u001b[39m, {})\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mmetatags\u001b[39m\u001b[39m'\u001b[39m, [{}])[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mog:updated_time\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     30\u001b[0m     })\n\u001b[0;32m     32\u001b[0m \u001b[39m# Save data to an Excel file\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'items'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def search_google(query, api_key, cx, num_results=10):\n",
    "    url = 'https://www.googleapis.com/customsearch/v1'\n",
    "    params = {\n",
    "        'key': api_key,\n",
    "        'cx': cx,\n",
    "        'q': query,\n",
    "        'num': num_results,\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    return response.json()\n",
    "\n",
    "api_key = 'AIzaSyCIGuLmSrM65sXknalTE4B4x8PsCZpAZ-I'\n",
    "cx = 'd4c72f123415549d9'\n",
    "query = 'ChatGPT use cases 2023'\n",
    "num_results = 10\n",
    "data = search_google(query, api_key, cx, num_results)\n",
    "\n",
    "# Extract relevant information\n",
    "results = []\n",
    "for item in data['items']:\n",
    "    results.append({\n",
    "        'title': item['title'],\n",
    "        'link': item['link'],\n",
    "        'snippet': item['snippet'],\n",
    "        'date': item.get('pagemap', {}).get('metatags', [{}])[0].get('og:updated_time')\n",
    "    })\n",
    "\n",
    "# Save data to an Excel file\n",
    "df = pd.DataFrame(results)\n",
    "df.to_excel('search_results.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'search_results.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[39mreturn\u001b[39;00m soup\u001b[39m.\u001b[39mprettify(formatter\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mxml\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[39m# Read the Excel file\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_excel(\u001b[39m'\u001b[39;49m\u001b[39msearch_results.xlsx\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     12\u001b[0m \u001b[39m# Scrape web pages and save them in XML format\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39mfor\u001b[39;00m index, row \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39miterrows():\n",
      "File \u001b[1;32mc:\\Users\\jaden\\OneDrive\\Coding\\Program\\Python 3.11\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jaden\\OneDrive\\Coding\\Program\\Python 3.11\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jaden\\OneDrive\\Coding\\Program\\Python 3.11\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:482\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    481\u001b[0m     should_close \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 482\u001b[0m     io \u001b[39m=\u001b[39m ExcelFile(io, storage_options\u001b[39m=\u001b[39;49mstorage_options, engine\u001b[39m=\u001b[39;49mengine)\n\u001b[0;32m    483\u001b[0m \u001b[39melif\u001b[39;00m engine \u001b[39mand\u001b[39;00m engine \u001b[39m!=\u001b[39m io\u001b[39m.\u001b[39mengine:\n\u001b[0;32m    484\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    485\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mEngine should not be specified when passing \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    486\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    487\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\jaden\\OneDrive\\Coding\\Program\\Python 3.11\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1652\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[0;32m   1650\u001b[0m     ext \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxls\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1651\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1652\u001b[0m     ext \u001b[39m=\u001b[39m inspect_excel_format(\n\u001b[0;32m   1653\u001b[0m         content_or_path\u001b[39m=\u001b[39;49mpath_or_buffer, storage_options\u001b[39m=\u001b[39;49mstorage_options\n\u001b[0;32m   1654\u001b[0m     )\n\u001b[0;32m   1655\u001b[0m     \u001b[39mif\u001b[39;00m ext \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1656\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1657\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mExcel file format cannot be determined, you must specify \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1658\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39man engine manually.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1659\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\jaden\\OneDrive\\Coding\\Program\\Python 3.11\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1525\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(content_or_path, \u001b[39mbytes\u001b[39m):\n\u001b[0;32m   1523\u001b[0m     content_or_path \u001b[39m=\u001b[39m BytesIO(content_or_path)\n\u001b[1;32m-> 1525\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[0;32m   1526\u001b[0m     content_or_path, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m, storage_options\u001b[39m=\u001b[39;49mstorage_options, is_text\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[0;32m   1527\u001b[0m ) \u001b[39mas\u001b[39;00m handle:\n\u001b[0;32m   1528\u001b[0m     stream \u001b[39m=\u001b[39m handle\u001b[39m.\u001b[39mhandle\n\u001b[0;32m   1529\u001b[0m     stream\u001b[39m.\u001b[39mseek(\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jaden\\OneDrive\\Coding\\Program\\Python 3.11\\Lib\\site-packages\\pandas\\io\\common.py:865\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    861\u001b[0m             newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(handle, ioargs\u001b[39m.\u001b[39;49mmode)\n\u001b[0;32m    866\u001b[0m     handles\u001b[39m.\u001b[39mappend(handle)\n\u001b[0;32m    868\u001b[0m \u001b[39m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'search_results.xlsx'"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def extract_content(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    return soup.prettify(formatter='xml')\n",
    "\n",
    "# Read the Excel file\n",
    "df = pd.read_excel('search_results.xlsx')\n",
    "\n",
    "# Scrape web pages and save them in XML format\n",
    "for index, row in df.iterrows():\n",
    "    content = extract_content(row['link'])\n",
    "    with open(f\"web_page_{index}.xml\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def summarize(text):\n",
    "    prompt = f\"Please provide a brief summary of the following text:\\n{text}\"\n",
    "    response = openai.Completion.create(\n",
    "        model=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=100,  # Adjust this value based on your desired summary length\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.5,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].text.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.lib import colors\n",
    "from reportlab.pdfgen import canvas\n",
    "\n",
    "# Summarize and save to PDF\n",
    "c = canvas.Canvas(\"summaries.pdf\", pagesize=letter)\n",
    "textobject = c.beginText()\n",
    "\n",
    "textobject.setTextOrigin(50, 750)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    with open(f\"web_page_{index}.xml\", \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "    summary = summarize(content)\n",
    "\n",
    "    # Write to PDF\n",
    "    textobject.setFont(\"Helvetica-Bold\", 14)\n",
    "    textobject.setFillColor(colors.red)\n",
    "    textobject.textOut(f\"Title: {row['title']}\\n\")\n",
    "    textobject.setFont(\"Helvetica\", 12)\n",
    "    textobject.setFillColor(colors.black)\n",
    "    textobject.textOut(f\"Link: {row['link']}\\n\")\n",
    "    textobject.textOut(f\"Summary: {summary}\\n\\n\")\n",
    "\n",
    "    # Check if the text reaches the end of the page\n",
    "    if textobject.getY() < 150:\n",
    "        c.drawText(textobject)\n",
    "        c.showPage()\n",
    "        textobject = c.beginText()\n",
    "        textobject.setTextOrigin(50, 750)\n",
    "\n",
    "# Save and close the PDF\n",
    "c.drawText(textobject)\n",
    "c.showPage()\n",
    "c.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "This model's maximum context length is 4097 tokens, however you requested 4331 tokens (4231 in your prompt; 100 for the completion). Please reduce your prompt; or completion length.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 136\u001b[0m\n\u001b[0;32m    133\u001b[0m     save_summaries_to_pdf(df)\n\u001b[0;32m    135\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 136\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[8], line 133\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    130\u001b[0m     save_scraped_data_to_file(index, content)\n\u001b[0;32m    132\u001b[0m \u001b[39m# Step 3\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m save_summaries_to_pdf(df)\n",
      "Cell \u001b[1;32mIn[8], line 84\u001b[0m, in \u001b[0;36msave_summaries_to_pdf\u001b[1;34m(df, filename)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mweb_page_\u001b[39m\u001b[39m{\u001b[39;00mindex\u001b[39m}\u001b[39;00m\u001b[39m.xml\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m     83\u001b[0m     content \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mread()\n\u001b[1;32m---> 84\u001b[0m summary \u001b[39m=\u001b[39m summarize(content)\n\u001b[0;32m     86\u001b[0m textobject\u001b[39m.\u001b[39msetFont(\u001b[39m\"\u001b[39m\u001b[39mHelvetica-Bold\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m14\u001b[39m)\n\u001b[0;32m     87\u001b[0m textobject\u001b[39m.\u001b[39msetFillColor(colors\u001b[39m.\u001b[39mred)\n",
      "Cell \u001b[1;32mIn[8], line 49\u001b[0m, in \u001b[0;36msummarize\u001b[1;34m(text, max_tokens)\u001b[0m\n\u001b[0;32m     46\u001b[0m truncated_text \u001b[39m=\u001b[39m truncate_text(text, max_tokens)\n\u001b[0;32m     48\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPlease provide a brief summary of the following text:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mtruncated_text\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 49\u001b[0m response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[0;32m     50\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtext-davinci-003\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     51\u001b[0m     prompt\u001b[39m=\u001b[39;49mprompt,\n\u001b[0;32m     52\u001b[0m     max_tokens\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[0;32m     53\u001b[0m     n\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m     54\u001b[0m     stop\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     55\u001b[0m     temperature\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m,\n\u001b[0;32m     56\u001b[0m )\n\u001b[0;32m     57\u001b[0m \u001b[39mreturn\u001b[39;00m response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mstrip()\n",
      "File \u001b[1;32mc:\\Users\\jaden\\OneDrive\\Coding\\Program\\Python 3.11\\Lib\\site-packages\\openai\\api_resources\\completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[1;32mc:\\Users\\jaden\\OneDrive\\Coding\\Program\\Python 3.11\\Lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    155\u001b[0m         url,\n\u001b[0;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32mc:\\Users\\jaden\\OneDrive\\Coding\\Program\\Python 3.11\\Lib\\site-packages\\openai\\api_requestor.py:226\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    206\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    207\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    214\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    215\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m    216\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[0;32m    217\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[0;32m    218\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    224\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[0;32m    225\u001b[0m     )\n\u001b[1;32m--> 226\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[0;32m    227\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[1;32mc:\\Users\\jaden\\OneDrive\\Coding\\Program\\Python 3.11\\Lib\\site-packages\\openai\\api_requestor.py:619\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    611\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    612\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    613\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    614\u001b[0m         )\n\u001b[0;32m    615\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[0;32m    616\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    617\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    618\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 619\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[0;32m    620\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    621\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[0;32m    622\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    623\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    624\u001b[0m         ),\n\u001b[0;32m    625\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    626\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\jaden\\OneDrive\\Coding\\Program\\Python 3.11\\Lib\\site-packages\\openai\\api_requestor.py:682\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    680\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[0;32m    681\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[1;32m--> 682\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[0;32m    683\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mInvalidRequestError\u001b[0m: This model's maximum context length is 4097 tokens, however you requested 4331 tokens (4231 in your prompt; 100 for the completion). Please reduce your prompt; or completion length."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import openai\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.lib import colors\n",
    "from reportlab.pdfgen import canvas\n",
    "\n",
    "# # Truncate the text function\n",
    "# def truncate_text(text, max_tokens=3900):\n",
    "#     tokens = text.split()\n",
    "#     if len(tokens) > max_tokens:\n",
    "#         tokens = tokens[:max_tokens]\n",
    "#     return \" \".join(tokens)\n",
    "\n",
    "# Step 1: Collect and save data\n",
    "def search_google(query, api_key, cx, num_results):\n",
    "    url = 'https://www.googleapis.com/customsearch/v1'\n",
    "    params = {\n",
    "        'key': api_key,\n",
    "        'cx': cx,\n",
    "        'q': query,\n",
    "        'num': num_results,\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    return response.json()\n",
    "\n",
    "def save_search_results_to_excel(results, filename='search_results.xlsx'):\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_excel(filename, index=False)\n",
    "\n",
    "# Step 2: Web scraping\n",
    "def scrape_webpage(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup.get_text()\n",
    "\n",
    "def save_scraped_data_to_file(index, content, file_format='xml'):\n",
    "    with open(f\"web_page_{index}.{file_format}\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "        \n",
    "# Step 2.5: Split the text into chunks of 1/4 of the maximum token limit\n",
    "def split_text(text, max_tokens=3900):\n",
    "    tokens = text.split()\n",
    "    num_chunks = int(np.ceil(len(tokens) / max_tokens))\n",
    "    chunks = []\n",
    "    for i in range(num_chunks):\n",
    "        chunks.append(\" \".join(tokens[i * max_tokens: (i + 1) * max_tokens]))\n",
    "    return chunks \n",
    "\n",
    "# # Step 3: Summarize using ChatGPT API + Truncate the text if it exceeds the maximum token limit\n",
    "# def summarize(text, max_tokens = 3900):\n",
    "#     # Truncate the text if it exceeds the maximum token limit\n",
    "#     truncated_text = truncate_text(text, max_tokens)\n",
    "\n",
    "#     prompt = f\"Please provide a brief summary of the following text:\\n{truncated_text}\"\n",
    "#     response = openai.Completion.create(\n",
    "#         model=\"text-davinci-003\",\n",
    "#         prompt=prompt,\n",
    "#         max_tokens=100,\n",
    "#         n=1,\n",
    "#         stop=None,\n",
    "#         temperature=0.5,\n",
    "#     )\n",
    "#     return response.choices[0].text.strip()\n",
    "\n",
    "# Step 3: Summarize using ChatGPT API\n",
    "def summarize(text, max_tokens = 4000):\n",
    "        # Truncate the text if it exceeds the maximum token limit\n",
    "    tokens = text.split()\n",
    "    truncated_text = \" \".join(tokens[:max_tokens])\n",
    "\n",
    "    prompt = f\"Please provide a brief summary of the following text:\\n{truncated_text}\"\n",
    "    response = openai.Completion.create(\n",
    "        model=\"gpt-4\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=100,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.5,\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "def save_summaries_to_pdf(df, filename='summaries.pdf'):\n",
    "    c = canvas.Canvas(filename, pagesize=letter)\n",
    "    textobject = c.beginText()\n",
    "    textobject.setTextOrigin(50, 750)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        with open(f\"web_page_{index}.xml\", \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "        summary = summarize(content)\n",
    "\n",
    "        textobject.setFont(\"Helvetica-Bold\", 14)\n",
    "        textobject.setFillColor(colors.red)\n",
    "        textobject.textOut(f\"Title: {row['title']}\\n\")\n",
    "        textobject.setFont(\"Helvetica\", 12)\n",
    "        textobject.setFillColor(colors.black)\n",
    "        textobject.textOut(f\"Link: {row['link']}\\n\")\n",
    "        textobject.textOut(f\"Summary: {summary}\\n\\n\")\n",
    "\n",
    "        if textobject.getY() < 150:\n",
    "            c.drawText(textobject)\n",
    "            c.showPage()\n",
    "            textobject = c.beginText()\n",
    "            textobject.setTextOrigin(50, 750)\n",
    "\n",
    "    c.drawText(textobject)\n",
    "    c.showPage()\n",
    "    c.save()\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # API keys and parameters\n",
    "    openai.api_key = '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' #os.getenv(\"OPENAI_API_KEY\")\n",
    "    api_key = 'AIzaSyCIGuLmSrM65sXknalTE4B4x8PsCZpAZ-I'\n",
    "    cx = 'd4c72f123415549d9'\n",
    "    query = 'ChatGPT use cases 2023'\n",
    "    num_results = 3\n",
    "\n",
    "    # Step 1\n",
    "    data = search_google(query, api_key, cx, num_results)\n",
    "    results = [\n",
    "        {\n",
    "            'title': item['title'],\n",
    "            'link': item['link'],\n",
    "            'snippet': item['snippet'],\n",
    "            'date': item.get('pagemap', {}).get('metatags', [{}])[0].get('og:updated_time')\n",
    "        }\n",
    "        for item in data['items']\n",
    "    ]\n",
    "    save_search_results_to_excel(results)\n",
    "\n",
    "    # Step 2\n",
    "    df = pd.read_excel('search_results.xlsx')\n",
    "    for index, row in df.iterrows():\n",
    "        content = scrape_webpage(row['link'])\n",
    "        save_scraped_data_to_file(index, content)\n",
    "\n",
    "    # Step 3\n",
    "    save_summaries_to_pdf(df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import openai\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.lib import colors\n",
    "from reportlab.pdfgen import canvas\n",
    "import getpass\n",
    "\n",
    "# # Truncate the text function\n",
    "# def truncate_text(text, max_tokens=3900):\n",
    "#     tokens = text.split()\n",
    "#     if len(tokens) > max_tokens:\n",
    "#         tokens = tokens[:max_tokens]\n",
    "#     return \" \".join(tokens)\n",
    "\n",
    "# Step 1: Collect and save data\n",
    "def search_google(query, api_key, cx, num_results):\n",
    "    url = 'https://www.googleapis.com/customsearch/v1'\n",
    "    params = {\n",
    "        'key': api_key,\n",
    "        'cx': cx,\n",
    "        'q': query,\n",
    "        'num': num_results,\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    return response.json()\n",
    "\n",
    "def save_search_results_to_excel(results, filename='search_results.xlsx'):\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_excel(filename, index=False)\n",
    "\n",
    "# Step 2: Web scraping\n",
    "def scrape_webpage(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup.get_text()\n",
    "\n",
    "def save_scraped_data_to_file(index, content, file_format='xml'):\n",
    "    with open(f\"web_page_{index}.{file_format}\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2.5: Split the text into chunks of 1/4 of the maximum token limit\n",
    "def split_text(text, max_tokens=3900):\n",
    "    tokens = text.split()\n",
    "    num_chunks = int(np.ceil(len(tokens) / max_tokens))\n",
    "    chunks = []\n",
    "    for i in range(num_chunks):\n",
    "        chunks.append(\" \".join(tokens[i * max_tokens: (i + 1) * max_tokens]))\n",
    "    return chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import openai\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.lib import colors\n",
    "from reportlab.pdfgen import canvas\n",
    "\n",
    "# Step 1: Collect and save data\n",
    "def search_google(query, api_key, cx, num_results):\n",
    "    url = 'https://www.googleapis.com/customsearch/v1'\n",
    "    params = {\n",
    "        'key': api_key,\n",
    "        'cx': cx,\n",
    "        'q': query,\n",
    "        'num': num_results,\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    return response.json()\n",
    "\n",
    "def save_search_results_to_excel(results, filename='search_results.xlsx'):\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_excel(filename, index=False)\n",
    "\n",
    "# Step 2: Web scraping\n",
    "def scrape_webpage(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup.get_text()\n",
    "\n",
    "def save_scraped_data_to_file(index, content, file_format='xml'):\n",
    "    with open(f\"web_page_{index}.{file_format}\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "# Step 3: Split and Summarize using ChatGPT API\n",
    "def split_and_summarize(text, num_parts=3):\n",
    "    text_length = len(text)\n",
    "    max_length = text_length // num_parts\n",
    "\n",
    "    parts = [text[i:i + max_length] for i in range(0, text_length, max_length)]\n",
    "\n",
    "    summaries = []\n",
    "\n",
    "    for part in parts:\n",
    "        response = openai.Completion.create(\n",
    "            model=\"text-davinci-003\",\n",
    "            prompt=f\"Please provide a brief summary of the following text:\\n{part}\",\n",
    "            max_tokens=100,\n",
    "            temperature=0.5\n",
    "        )\n",
    "        summaries.append(response.choices[0].text.strip())\n",
    "\n",
    "    aggregated = ' '.join(summaries)\n",
    "    \n",
    "    final_response = openai.Completion.create(\n",
    "        model=\"text-davinci-003\",\n",
    "        prompt=f\"Please provide a brief summary of the following text:\\n{aggregated}\",\n",
    "        max_tokens=100,\n",
    "        temperature=0.5\n",
    "    )\n",
    "\n",
    "    return final_response.choices[0].text.strip()\n",
    "\n",
    "def save_summaries_to_pdf(df, filename='summaries.pdf'):\n",
    "    c = canvas.Canvas(filename, pagesize=letter)\n",
    "    textobject = c.beginText()\n",
    "    textobject.setTextOrigin(50, 750)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        with open(f\"web_page_{index}.xml\", \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "        summary = split_and_summarize(content)\n",
    "\n",
    "        textobject.setFont(\"Helvetica-Bold\", 14)\n",
    "        textobject.setFillColor(colors.red)\n",
    "        textobject.textOut(f\"Title: {row['title']}\\n\")\n",
    "        textobject.setFont(\"Helvetica\", 12)\n",
    "        textobject.setFillColor(colors.black)\n",
    "        textobject.textOut(f\"Link: {row['link']}\\n\")\n",
    "        textobject.textOut(f\"Summary: {summary}\\n\\n\")\n",
    "\n",
    "        if textobject.getY() < 150:\n",
    "            c.drawText(textobject)\n",
    "            c.showPage()\n",
    "            textobject = c.beginText()\n",
    "            textobject.setTextOrigin(50, 750)\n",
    "\n",
    "    c.drawText(textobject)\n",
    "    c.showPage()\n",
    "    c.save()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'summarize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m     save_summaries_to_pdf(df)\n\u001b[0;32m     38\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> 39\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[5], line 31\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m content \u001b[39m=\u001b[39m scrape_webpage(row[\u001b[39m'\u001b[39m\u001b[39mlink\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     30\u001b[0m chunks \u001b[39m=\u001b[39m split_and_summarize(content)\n\u001b[1;32m---> 31\u001b[0m summarized_chunks \u001b[39m=\u001b[39m [summarize(chunk) \u001b[39mfor\u001b[39;49;00m chunk \u001b[39min\u001b[39;49;00m chunks]\n\u001b[0;32m     32\u001b[0m content \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(summarized_chunks)\n\u001b[0;32m     33\u001b[0m save_scraped_data_to_file(index, content)\n",
      "Cell \u001b[1;32mIn[5], line 31\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     29\u001b[0m content \u001b[39m=\u001b[39m scrape_webpage(row[\u001b[39m'\u001b[39m\u001b[39mlink\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     30\u001b[0m chunks \u001b[39m=\u001b[39m split_and_summarize(content)\n\u001b[1;32m---> 31\u001b[0m summarized_chunks \u001b[39m=\u001b[39m [summarize(chunk) \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m chunks]\n\u001b[0;32m     32\u001b[0m content \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(summarized_chunks)\n\u001b[0;32m     33\u001b[0m save_scraped_data_to_file(index, content)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'summarize' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # API keys and parameters\n",
    "    openai.api_key = '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' #os.getenv(\"OPENAI_API_KEY\")\n",
    "    api_key = 'AIzaSyCIGuLmSrM65sXknalTE4B4x8PsCZpAZ-I'\n",
    "    cx = 'd4c72f123415549d9'\n",
    "    # openai.api_key = os.getenv(\"openai_api_key\")\n",
    "    # api_key = os.getenv('google_api_key')\n",
    "    # cx = os.getenv('google_cx')\n",
    "    query = 'ChatGPT use cases 2023'\n",
    "    num_results = 3\n",
    "\n",
    "    # Step 1\n",
    "    data = search_google(query, api_key, cx, num_results)\n",
    "    results = [\n",
    "        {\n",
    "            'title': item['title'],\n",
    "            'link': item['link'],\n",
    "            'snippet': item['snippet'],\n",
    "            'date': item.get('pagemap', {}).get('metatags', [{}])[0].get('og:updated_time')\n",
    "        }\n",
    "        for item in data['items']\n",
    "    ]\n",
    "    save_search_results_to_excel(results)\n",
    "\n",
    "    # Step 2\n",
    "    df = pd.read_excel('search_results.xlsx')\n",
    "    for index, row in df.iterrows():\n",
    "        content = scrape_webpage(row['link'])\n",
    "        chunks = split_and_summarize(content)\n",
    "        summarized_chunks = [summarize(chunk) for chunk in chunks]\n",
    "        content = ' '.join(summarized_chunks)\n",
    "        save_scraped_data_to_file(index, content)\n",
    "\n",
    "    # Step 3\n",
    "    save_summaries_to_pdf(df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import openai\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.lib import colors\n",
    "from reportlab.pdfgen import canvas\n",
    "\n",
    "# Step 1: Collect and save data\n",
    "def search_google(query, api_key, cx, num_results):\n",
    "    url = 'https://www.googleapis.com/customsearch/v1'\n",
    "    params = {\n",
    "        'key': api_key,\n",
    "        'cx': cx,\n",
    "        'q': query,\n",
    "        'num': num_results,\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    return response.json()\n",
    "\n",
    "def save_search_results_to_excel(results, filename='search_results.xlsx'):\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_excel(filename, index=False)\n",
    "\n",
    "# Step 2: Web scraping\n",
    "def scrape_webpage(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup.get_text()\n",
    "\n",
    "def save_scraped_data_to_file(index, content, file_format='xml'):\n",
    "    with open(f\"web_page_{index}.{file_format}\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "# Step 3: Summarize using ChatGPT API\n",
    "def summarize(text, max_tokens = 4000):\n",
    "    tokens = text.split()\n",
    "    truncated_text = \" \".join(tokens[:max_tokens])\n",
    "\n",
    "    prompt = f\"Please provide a brief summary of the following text:\\n{truncated_text}\"\n",
    "    response = openai.Completion.create(\n",
    "        model=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=300,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.5,\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "def split_and_summarize(text, num_parts=5):\n",
    "    text_length = len(text)\n",
    "    max_length = text_length // num_parts\n",
    "\n",
    "    parts = [text[i:i + max_length] for i in range(0, text_length, max_length)]\n",
    "\n",
    "    summaries = []\n",
    "\n",
    "    for part in parts:\n",
    "        summary = summarize(part)\n",
    "        summaries.append(summary)\n",
    "\n",
    "    aggregated = ' '.join(summaries)\n",
    "    final_summary = summarize(aggregated)\n",
    "\n",
    "    return final_summary\n",
    "\n",
    "def save_summaries_to_pdf(df, filename='summaries.pdf'):\n",
    "    c = canvas.Canvas(filename, pagesize=letter)\n",
    "    textobject = c.beginText()\n",
    "    textobject.setTextOrigin(50, 750)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        with open(f\"web_page_{index}.xml\", \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "        summary = summarize(content)\n",
    "\n",
    "        textobject.setFont(\"Helvetica-Bold\", 14)\n",
    "        textobject.setFillColor(colors.red)\n",
    "        textobject.textOut(f\"Title: {row['title']}\\n\")\n",
    "        textobject.setFont(\"Helvetica\", 12)\n",
    "        textobject.setFillColor(colors.black)\n",
    "        textobject.textOut(f\"Link: {row['link']}\\n\")\n",
    "        textobject.textOut(f\"Summary: {summary}\\n\\n\")\n",
    "\n",
    "        if textobject.getY() < 300:\n",
    "            c.drawText(textobject)\n",
    "            c.showPage()\n",
    "            textobject = c.beginText()\n",
    "            textobject.setTextOrigin(50, 750)\n",
    "\n",
    "    c.drawText(textobject)\n",
    "    c.showPage()\n",
    "    c.save()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'web_page_0.xml'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m     save_summaries_to_pdf(df)\n\u001b[0;32m     43\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> 44\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[17], line 41\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m     save_scraped_data_to_file(index, summarized_content, \u001b[39m'\u001b[39m\u001b[39mtxt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[39m# # Step 2\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[39m# df = pd.read_excel('search_results.xlsx')\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[39m# for index, row in df.iterrows():\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m \n\u001b[0;32m     40\u001b[0m \u001b[39m# Step 3\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m save_summaries_to_pdf(df)\n",
      "Cell \u001b[1;32mIn[16], line 76\u001b[0m, in \u001b[0;36msave_summaries_to_pdf\u001b[1;34m(df, filename)\u001b[0m\n\u001b[0;32m     73\u001b[0m textobject\u001b[39m.\u001b[39msetTextOrigin(\u001b[39m50\u001b[39m, \u001b[39m750\u001b[39m)\n\u001b[0;32m     75\u001b[0m \u001b[39mfor\u001b[39;00m index, row \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39miterrows():\n\u001b[1;32m---> 76\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mweb_page_\u001b[39;49m\u001b[39m{\u001b[39;49;00mindex\u001b[39m}\u001b[39;49;00m\u001b[39m.xml\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m, encoding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m     77\u001b[0m         content \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mread()\n\u001b[0;32m     78\u001b[0m     summary \u001b[39m=\u001b[39m summarize(content)\n",
      "File \u001b[1;32mc:\\Users\\jaden\\OneDrive\\Coding\\Program\\Python 3.11\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'web_page_0.xml'"
     ]
    }
   ],
   "source": [
    "# Main function\n",
    "def main():\n",
    "    # API keys and parameters\n",
    "    openai.api_key = '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' #os.getenv(\"OPENAI_API_KEY\")\n",
    "    api_key = 'AIzaSyCIGuLmSrM65sXknalTE4B4x8PsCZpAZ-I'\n",
    "    cx = 'd4c72f123415549d9'\n",
    "    query = 'ChatGPT use cases 2023'\n",
    "    num_results = 5\n",
    "\n",
    "    # Step 1\n",
    "    data = search_google(query, api_key, cx, num_results)\n",
    "    if 'items' in data:\n",
    "        results = [\n",
    "            {\n",
    "                'title': item['title'],\n",
    "                'link': item['link'],\n",
    "                'snippet': item['snippet'],\n",
    "                'date': item.get('pagemap', {}).get('metatags', [{}])[0].get('og:updated_time')\n",
    "            }\n",
    "            for item in data['items']\n",
    "        ]\n",
    "        save_search_results_to_excel(results)\n",
    "    else:\n",
    "        print(\"No items in the response. Please check your query or Google Custom Search setup.\")\n",
    "\n",
    "    # Step 2\n",
    "    df = pd.read_excel('search_results.xlsx')\n",
    "    for index, row in df.iterrows():\n",
    "        content = scrape_webpage(row['link'])\n",
    "        summarized_content = split_and_summarize(content)\n",
    "        save_scraped_data_to_file(index, summarized_content, 'txt')\n",
    "        \n",
    "    # # Step 2\n",
    "    # df = pd.read_excel('search_results.xlsx')\n",
    "    # for index, row in df.iterrows():\n",
    "    #     content = scrape_webpage(row['link'])\n",
    "    #     summarized_content = summarize(content)\n",
    "    #     save_scraped_data_to_file(index, summarized_content, 'txt')\n",
    "\n",
    "    # Step 3\n",
    "    save_summaries_to_pdf(df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import openai\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.lib import colors\n",
    "from reportlab.pdfgen import canvas\n",
    "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
    "from scipy import spatial\n",
    "\n",
    "class WebScraper:\n",
    "    def __init__(self, api_key, cx, query, num_results):\n",
    "        self.api_key = api_key\n",
    "        self.cx = cx\n",
    "        self.query = query\n",
    "        self.num_results = num_results\n",
    "        self.results = []\n",
    "\n",
    "    def search_google(self):\n",
    "        url = 'https://www.googleapis.com/customsearch/v1'\n",
    "        params = {\n",
    "            'key': self.api_key,\n",
    "            'cx': self.cx,\n",
    "            'q': self.query,\n",
    "            'num': self.num_results,\n",
    "        }\n",
    "        response = requests.get(url, params=params)\n",
    "        return response.json()\n",
    "\n",
    "    def save_search_results_to_excel(self, filename='search_results.xlsx'):\n",
    "        df = pd.DataFrame(self.results)\n",
    "        df.to_excel(filename, index=False)\n",
    "\n",
    "    def scrape_webpage(self, url):\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        return soup.get_text()\n",
    "\n",
    "    def save_scraped_data_to_file(self, index, content, file_format='xml'):\n",
    "        with open(f\"web_page_{index}.{file_format}\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(content)\n",
    "\n",
    "    def summarize(self, text, max_tokens = 4000):\n",
    "        tokens = text.split()\n",
    "        truncated_text = \" \".join(tokens[:max_tokens])\n",
    "\n",
    "        prompt = f\"Please provide a brief summary of the following text:\\n{truncated_text}\"\n",
    "        response = openai.Completion.create(\n",
    "            model=\"text-davinci-003\",\n",
    "            prompt=prompt,\n",
    "            max_tokens=300,\n",
    "            n=1,\n",
    "            stop=None,\n",
    "            temperature=0.5,\n",
    "        )\n",
    "        return response.choices[0].text.strip()\n",
    "\n",
    "    def split_and_summarize(self, text, num_parts=5):\n",
    "        text_length = len(text)\n",
    "        max_length = text_length // num_parts\n",
    "\n",
    "        parts = [text[i:i + max_length] for i in range(0, text_length, max_length)]\n",
    "\n",
    "        summaries = []\n",
    "\n",
    "        for part in parts:\n",
    "            summary = self.summarize(part)\n",
    "            summaries.append(summary)\n",
    "\n",
    "        aggregated = ' '.join(summaries)\n",
    "        final_summary = self.summarize(aggregated)\n",
    "\n",
    "        return final_summary\n",
    "\n",
    "    def save_summaries_to_pdf(self, df, filename='summaries.pdf'):\n",
    "        c = canvas.Canvas(filename, pagesize=letter)\n",
    "        textobject = c.beginText()\n",
    "        textobject.setTextOrigin(50, 750)\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            with open(f\"web_page_{index}.xml\", \"r\", encoding=\"utf-8\") as f:\n",
    "                content = f.read()\n",
    "            summary = self.summarize(content)\n",
    "\n",
    "            textobject.setFont(\"Helvetica-Bold\", 14)\n",
    "            textobject.setFillColor(colors.red)\n",
    "            textobject.textOut(f\"Title: {row['title']}\\n\")\n",
    "            textobject.setFont(\"Helvetica\", 12)\n",
    "            textobject.setFillColor(colors.black)\n",
    "            textobject.textOut(f\"Link: {row['link']}\\n\")\n",
    "            textobject.textOut(f\"Summary: {summary}\\n\\n\")\n",
    "\n",
    "            if textobject.getY() < 300:\n",
    "                c.drawText(textobject)\n",
    "                c.showPage()\n",
    "                textobject = c.beginText()\n",
    "                textobject.setTextOrigin(50, 750)\n",
    "                c.drawText(textobject)\n",
    "                c.showPage()\n",
    "                c.save()\n",
    "\n",
    "    def run(self):\n",
    "        # Step 1\n",
    "        data = self.search_google()\n",
    "        if 'items' in data:\n",
    "            self.results = [\n",
    "                {\n",
    "                    'title': item['title'],\n",
    "                    'link': item['link'],\n",
    "                    'snippet': item['snippet'],\n",
    "                    'date': item.get('pagemap', {}).get('metatags', [{}])[0].get('og:updated_time')\n",
    "                }\n",
    "                for item in data['items']\n",
    "            ]\n",
    "            self.save_search_results_to_excel()\n",
    "        else:\n",
    "            print(\"No items in the response. Please check your query or Google Custom Search setup.\")\n",
    "\n",
    "        # Step 2\n",
    "        df = pd.read_excel('search_results.xlsx')\n",
    "        for index, row in df.iterrows():\n",
    "            content = self.scrape_webpage(row['link'])\n",
    "            summarized_content = self.split_and_summarize(content)\n",
    "            self.save_scraped_data_to_file(index, summarized_content, 'txt')\n",
    "\n",
    "        # Step 3\n",
    "        self.save_summaries_to_pdf(df)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    api_key = 'AIzaSyCIGuLmSrM65sXknalTE4B4x8PsCZpAZ-I'\n",
    "    cx = 'd4c72f123415549d9'\n",
    "    query = 'ChatGPT use cases 2023'\n",
    "    num_results = 5\n",
    "    scraper = WebScraper(api_key, cx, query, num_results)\n",
    "    scraper.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
