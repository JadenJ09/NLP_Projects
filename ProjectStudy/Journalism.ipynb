{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import openai\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.lib import colors\n",
    "from reportlab.pdfgen import canvas\n",
    "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
    "from scipy import spatial\n",
    "\n",
    "class WebScraper:\n",
    "    def __init__(self, api_key, cx, query, num_results):\n",
    "        self.api_key = api_key\n",
    "        self.cx = cx\n",
    "        self.query = query\n",
    "        self.num_results = num_results\n",
    "        self.results = []\n",
    "\n",
    "    def search_google(self):\n",
    "        url = 'https://www.googleapis.com/customsearch/v1'\n",
    "        params = {\n",
    "            'key': self.api_key,\n",
    "            'cx': self.cx,\n",
    "            'q': self.query,\n",
    "            'num': self.num_results,\n",
    "        }\n",
    "        response = requests.get(url, params=params)\n",
    "        return response.json()\n",
    "\n",
    "    def save_search_results_to_excel(self, filename='search_results.xlsx'):\n",
    "        df = pd.DataFrame(self.results)\n",
    "        df.to_excel(filename, index=False)\n",
    "\n",
    "    def scrape_webpage(self, url):\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        return soup.get_text()\n",
    "\n",
    "    def save_scraped_data_to_file(self, index, content, file_format='xml'):\n",
    "        with open(f\"web_page_{index}.{file_format}\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(content)\n",
    "\n",
    "    def summarize(self, text, max_tokens = 4000):\n",
    "        tokens = text.split()\n",
    "        truncated_text = \" \".join(tokens[:max_tokens])\n",
    "\n",
    "        prompt = f\"Please provide a brief summary of the following text:\\n{truncated_text}\"\n",
    "        response = openai.Completion.create(\n",
    "            model=\"text-davinci-003\",\n",
    "            prompt=prompt,\n",
    "            max_tokens=300,\n",
    "            n=1,\n",
    "            stop=None,\n",
    "            temperature=0.5,\n",
    "        )\n",
    "        return response.choices[0].text.strip()\n",
    "\n",
    "    def split_and_summarize(self, text, num_parts=5):\n",
    "        text_length = len(text)\n",
    "        max_length = text_length // num_parts\n",
    "\n",
    "        parts = [text[i:i + max_length] for i in range(0, text_length, max_length)]\n",
    "\n",
    "        summaries = []\n",
    "\n",
    "        for part in parts:\n",
    "            summary = self.summarize(part)\n",
    "            summaries.append(summary)\n",
    "\n",
    "        aggregated = ' '.join(summaries)\n",
    "        final_summary = self.summarize(aggregated)\n",
    "\n",
    "        return final_summary\n",
    "\n",
    "    def save_summaries_to_pdf(self, df, filename='summaries.pdf'):\n",
    "        c = canvas.Canvas(filename, pagesize=letter)\n",
    "        textobject = c.beginText()\n",
    "        textobject.setTextOrigin(50, 750)\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            with open(f\"web_page_{index}.xml\", \"r\", encoding=\"utf-8\") as f:\n",
    "                content = f.read()\n",
    "            summary = self.summarize(content)\n",
    "\n",
    "            textobject.setFont(\"Helvetica-Bold\", 14)\n",
    "            textobject.setFillColor(colors.red)\n",
    "            textobject.textOut(f\"Title: {row['title']}\\n\")\n",
    "            textobject.setFont(\"Helvetica\", 12)\n",
    "            textobject.setFillColor(colors.black)\n",
    "            textobject.textOut(f\"Link: {row['link']}\\n\")\n",
    "            textobject.textOut(f\"Summary: {summary}\\n\\n\")\n",
    "\n",
    "            if textobject.getY() < 300:\n",
    "                c.drawText(textobject)\n",
    "                c.showPage()\n",
    "                textobject = c.beginText()\n",
    "                textobject.setTextOrigin(50, 750)\n",
    "                c.drawText(textobject)\n",
    "                c.showPage()\n",
    "                c.save()\n",
    "\n",
    "    def run(self):\n",
    "        # Step 1\n",
    "        data = self.search_google()\n",
    "        if 'items' in data:\n",
    "            self.results = [\n",
    "                {\n",
    "                    'title': item['title'],\n",
    "                    'link': item['link'],\n",
    "                    'snippet': item['snippet'],\n",
    "                    'date': item.get('pagemap', {}).get('metatags', [{}])[0].get('og:updated_time')\n",
    "                }\n",
    "                for item in data['items']\n",
    "            ]\n",
    "            self.save_search_results_to_excel()\n",
    "        else:\n",
    "            print(\"No items in the response. Please check your query or Google Custom Search setup.\")\n",
    "\n",
    "        # Step 2\n",
    "        df = pd.read_excel('search_results.xlsx')\n",
    "        for index, row in df.iterrows():\n",
    "            content = self.scrape_webpage(row['link'])\n",
    "            summarized_content = self.split_and_summarize(content)\n",
    "            self.save_scraped_data_to_file(index, summarized_content, 'txt')\n",
    "\n",
    "        # Step 3\n",
    "        self.save_summaries_to_pdf(df)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    api_key = 'AIzaSyCIGuLmSrM65sXknalTE4B4x8PsCZpAZ-I'\n",
    "    cx = 'd4c72f123415549d9'\n",
    "    query = 'ChatGPT use cases 2023'\n",
    "    num_results = 5\n",
    "    scraper = WebScraper(api_key, cx, query, num_results)\n",
    "    scraper.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # def scrape_webpage(self, url):\n",
    "    #     response = requests.get(url)\n",
    "    #     soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    #     return soup.get_text()\n",
    "\n",
    "    # def save_scraped_data_to_file(self, index, content, file_format='xml'):\n",
    "    #     with open(f\"web_page_{index}.{file_format}\", \"w\", encoding=\"utf-8\") as f:\n",
    "    #         f.write(content)\n",
    "    \n",
    "    # def scrape_and_save(self):\n",
    "    #     def extract_content(url):\n",
    "    #         response = requests.get(url)\n",
    "    #         soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    #         return soup.prettify(formatter='xml')\n",
    "\n",
    "    #     def save_content_to_pdf(content, filename):\n",
    "    #         # Create a subfolder named 'pdfs' if it doesn't exist\n",
    "    #         if not os.path.exists(f'{self.query}_pdfs'):\n",
    "    #             os.makedirs(f'{self.query}_pdfs')\n",
    "\n",
    "    #         # Save the file in the 'pdfs' subfolder\n",
    "    #         with open(os.path.join(f'{self.query}_pdfs', filename), \"w\", encoding=\"utf-8\") as f:\n",
    "    #             f.write(content)\n",
    "\n",
    "    #     # Read the Excel file\n",
    "    #     df = pd.read_excel(f'{self.query}.xlsx')\n",
    "\n",
    "    #     # Scrape web pages and save them in pdf format\n",
    "    #     for index, row in df.iterrows():\n",
    "    #         content = extract_content(row['link'])\n",
    "    #         save_content_to_pdf(content, f\"web_page_{index}.pdf\")\n",
    "    \n",
    "    ## Using beautiful soup and pdfkit\n",
    "    # def scrape_and_save(self):\n",
    "    #     def extract_content(url):\n",
    "    #         headers = {\n",
    "    #             'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    #         }\n",
    "    #         response = requests.get(url, headers=headers)\n",
    "    #         soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    #         return soup.prettify(formatter='html')\n",
    "\n",
    "        \n",
    "    #     path_wkhtmltopdf = r'C:\\Program Files\\wkhtmltopdf\\bin\\wkhtmltopdf.exe'  # replace with your actual path\n",
    "    #     config = pdfkit.configuration(wkhtmltopdf=path_wkhtmltopdf)\n",
    "\n",
    "    #     def save_content_to_pdf(content, filename):\n",
    "    #         # Create a subfolder named 'pdfs' if it doesn't exist\n",
    "    #         if not os.path.exists(f'{self.query}_pdfs'):\n",
    "    #             os.makedirs(f'{self.query}_pdfs')\n",
    "\n",
    "    #         # # Save the file in the 'pdfs' subfolder\n",
    "    #         # with open(os.path.join(f'{self.query}_pdfs', filename), \"w\", encoding=\"utf-8\") as f:\n",
    "    #         #     f.write(content)\n",
    "            \n",
    "    #          # Define options for pdfkit\n",
    "    #         options = {\n",
    "    #             'no-stop-slow-scripts': True,\n",
    "    #             'load-error-handling': 'ignore'\n",
    "    #         }\n",
    "\n",
    "    #         # Save the file in the 'pdfs' subfolder\n",
    "    #         pdfkit.from_string(content, os.path.join(f'{self.query}_pdfs', filename), configuration=config, options=options)\n",
    "\n",
    "\n",
    "    #     # Read the Excel file\n",
    "    #     df = pd.read_excel(f'{self.query}.xlsx')\n",
    "\n",
    "    #     # Scrape web pages and save them in pdf format\n",
    "    #     for index, row in df.iterrows():\n",
    "    #         content = extract_content(row['link'])\n",
    "    #         save_content_to_pdf(content, f\"web_page_{index}.pdf\")\n",
    "            \n",
    "    ## Using beautiful soup and weasyprint        \n",
    "    # def scrape_and_save(self):\n",
    "        \n",
    "    #     def extract_content(url):\n",
    "    #         headers = {\n",
    "    #             'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    #         }\n",
    "    #         response = requests.get(url, headers=headers)\n",
    "    #         html = response.content.decode('utf-8')\n",
    "    #         content = HTML(string=html)\n",
    "    #         return content\n",
    "\n",
    "    #     def save_content_to_pdf(content, filename):\n",
    "    #         # Create a subfolder named 'pdfs' if it doesn't exist\n",
    "    #         if not os.path.exists(f'{self.query}_pdfs'):\n",
    "    #             os.makedirs(f'{self.query}_pdfs')\n",
    "\n",
    "    #         # Save the file in the 'pdfs' subfolder\n",
    "    #         HTML(string=content).write_pdf(os.path.join(f'{self.query}_pdfs', filename))\n",
    "\n",
    "    #     # Read the Excel file\n",
    "    #     df = pd.read_excel(f'{self.query}.xlsx')\n",
    "\n",
    "    #     # Scrape web pages and save them in pdf format\n",
    "    #     for index, row in df.iterrows():\n",
    "    #         content = extract_content(row['link'])\n",
    "    #         save_content_to_pdf(content, f\"web_page_{index}.pdf\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    def scrape_summarize_and_save(self):\n",
    "        df = pd.read_excel(f'{self.query}.xlsx')\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "            url = row['link']\n",
    "            title = row['title']\n",
    "            if url not in self.visited_urls:\n",
    "                content = self.extract_content(url)\n",
    "                summarized_content = self.summarize_text(content)\n",
    "                keywords = self.extract_keywords(summarized_content)\n",
    "                structured_content = f\"Article: {content}\\n <h2>Keywords:</h2> {keywords}\\n <h2>Summarized:</h2> {summarized_content}\"\n",
    "                self.save_content_to_pdf(title, url, structured_content, f\"web_page_{index}.pdf\")\n",
    "                self.visited_urls.add(url)\n",
    "            else:\n",
    "                print(f\"Skipping already visited URL: {url}\")\n",
    "                \n",
    "\n",
    "\n",
    "    def extract_text_from_pdfs_in_folder(self):\n",
    "        # ... existing code ...\n",
    "\n",
    "        for pdf_file in pdf_files:\n",
    "            # ... existing code ...\n",
    "\n",
    "            # Extract keywords and add to PDF text\n",
    "            keywords = self.extract_keywords(text)\n",
    "            text = f\"URL: {pdf_file}\\nKeywords: {', '.join(keywords)}\\n{text}\"\n",
    "\n",
    "            # Summarize text and add to PDF text\n",
    "            summary = self.summarize_text(text)\n",
    "            text = f\"{text}\\n\\nSummarized by LLM:\\n{summary}\"\n",
    "\n",
    "            # Calculate embedding and store\n",
    "            embedding = self.get_embedding(summary)\n",
    "            self.embeddings[pdf_file] = embedding\n",
    "\n",
    "            # ... existing code ...\n",
    "\n",
    "        return pdf_texts\n",
    "\n",
    "    def visualize_text(self):\n",
    "        # Convert embeddings to matrix\n",
    "        matrix = np.vstack(self.embeddings.values())\n",
    "\n",
    "        # Perform clustering\n",
    "        kmeans = KMeans(n_clusters=4, init='k-means++', random_state=42)\n",
    "        kmeans.fit(matrix)\n",
    "\n",
    "        # Create scatter plot of clusters\n",
    "        plt.scatter(matrix[:, 0], matrix[:, 1], c=kmeans.labels_)\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "            \n",
    "    def extract_text_from_pdfs_in_folder(self):\n",
    "        # Get a list of all PDF files in the folder\n",
    "        pdf_files = [f for f in os.listdir(self.folder_path) if f.endswith('.pdf')]\n",
    "\n",
    "        # Initialize a dictionary to hold the extracted text for each PDF\n",
    "        pdf_texts = {}\n",
    "\n",
    "        # Process each PDF file\n",
    "        for pdf_file in pdf_files:\n",
    "            # Open the PDF file\n",
    "            with open(os.path.join(self.folder_path, pdf_file), 'rb') as f:\n",
    "                # Create a PdfFileReader object\n",
    "                pdf = PdfReader(f)\n",
    "\n",
    "                # Initialize a list to hold the text from each page\n",
    "                text = []\n",
    "\n",
    "                # Extract the text from each page\n",
    "                for page in pdf.pages:\n",
    "                    text.append(page.extract_text())\n",
    "\n",
    "                # Join the text from all pages into a single string\n",
    "                pdf_texts[pdf_file] = ' '.join(text)\n",
    "\n",
    "        return pdf_texts\n",
    "\n",
    "\n",
    "\n",
    "    def save_summaries(self, summaries):\n",
    "        pdf = fpdf()\n",
    "        for title, summary in summaries.items():\n",
    "            pdf.add_page()\n",
    "            pdf.set_font(\"Arial\", size = 15)\n",
    "            pdf.cell(200, 10, txt = title, ln = True, align = 'C')\n",
    "            pdf.multi_cell(200, 10, txt = summary)\n",
    "        pdf.output(\"summarized_webpages.pdf\")\n",
    "\n",
    "# # Usage\n",
    "# folder_path = 'path_to_your_folder'\n",
    "# summarizer = PDFSummarizer(folder_path)\n",
    "# texts = summarizer.extract_text_from_pdfs_in_folder()\n",
    "# summaries = {title: summarizer.summarize_text(text) for title, text in texts.items()}\n",
    "# summarizer.save_summaries(summaries)\n",
    "\n",
    "    # def extract_text_from_pdfs_in_folder(self):\n",
    "    #     # Get a list of all PDF files in the folder\n",
    "    #     pdf_files = [f for f in os.listdir(self.folder_path) if f.endswith('.pdf')]\n",
    "\n",
    "    #     # Initialize a dictionary to hold the extracted text for each PDF\n",
    "    #     pdf_texts = {}\n",
    "\n",
    "    #     # Process each PDF file\n",
    "    #     for pdf_file in pdf_files:\n",
    "    #         # Open the PDF file\n",
    "    #         with open(os.path.join(self.folder_path, pdf_file), 'rb') as f:\n",
    "    #             # Create a PdfFileReader object\n",
    "    #             pdf = PdfFileReader(f)\n",
    "\n",
    "    #             # Initialize a list to hold the text from each page\n",
    "    #             text = []\n",
    "\n",
    "    #             # Extract the text from each page\n",
    "    #             for page_num in range(pdf.getNumPages()):\n",
    "    #                 text.append(pdf.getPage(page_num).extractText())\n",
    "\n",
    "    #             # Join the text from all pages into a single string\n",
    "    #             pdf_texts[pdf_file] = ' '.join(text)\n",
    "\n",
    "    #     return pdf_texts\n",
    "\n",
    "    # def summarize_text(self, text):\n",
    "    #     # Split the text into chunks of 3000 tokens each\n",
    "    #     tokens = text.split()\n",
    "    #     chunks = [' '.join(tokens[i:i + 3000]) for i in range(0, len(tokens), 3000)]\n",
    "\n",
    "    #     summaries = []\n",
    "    #     for chunk in chunks:\n",
    "    #         # Summarize each chunk using the OpenAI API\n",
    "    #         prompt = f\"Please provide a bit detailed summary of the following text with keywords max 300 words:\\n{chunk}\"\n",
    "    #         response = openai.Completion.create(\n",
    "    #             model=\"text-davinci-003\",\n",
    "    #             prompt=prompt,\n",
    "    #             max_tokens=300,\n",
    "    #             n=1,\n",
    "    #             stop=None,\n",
    "    #             temperature=0.5,\n",
    "    #         )\n",
    "    #         summaries.append(response.choices[0].text.strip())\n",
    "\n",
    "    #     # Aggregate the summaries\n",
    "    #     aggregated_summary = ' '.join(summaries)\n",
    "\n",
    "    #     # Summarize the aggregated summary\n",
    "    #     prompt = f\"The following text are aggregated summaries of a conference. Aggregate the text and produce an report article like Bloomberg article, max 1000 words with core keywords:\\n{aggregated_summary}\"\n",
    "    #     response = openai.Completion.create(\n",
    "    #         model=\"text-davinci-003\",\n",
    "    #         prompt=prompt,\n",
    "    #         max_tokens=500,\n",
    "    #         n=1,\n",
    "    #         stop=None,\n",
    "    #         temperature=0.5,\n",
    "    #     )\n",
    "    #     final_summary = response.choices[0].text.strip()\n",
    "\n",
    "    #     return final_summary\n",
    "\n",
    "    def extract_keywords(self, text):\n",
    "        n_gram_range = (3, 3)\n",
    "        stop_words = \"english\"\n",
    "\n",
    "        # Extract candidate words/phrases\n",
    "        count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([text])\n",
    "        candidates = count.get_feature_names()\n",
    "\n",
    "        return candidates\n",
    "\n",
    "    def get_embedding(self, text, model=\"text-embedding-ada-002\"):\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "        return openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']\n",
    "\n",
    "    def process_pdfs(self):\n",
    "        summaries = []\n",
    "        keywords = []\n",
    "        embeddings = []\n",
    "\n",
    "        for filename in os.listdir(self.folder_path):\n",
    "            if filename.endswith(\".pdf\"):\n",
    "                pdf_path = os.path.join(self.folder_path, filename)\n",
    "                text = self.extract_text_from_pdf(pdf_path)\n",
    "                summary = self.summarize_text(text)\n",
    "                keyword = self.extract_keywords(text)\n",
    "                embedding = self.get_embedding(text)\n",
    "\n",
    "                summaries.append(summary)\n",
    "                keywords.append(keyword)\n",
    "                embeddings.append(embedding)\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            'summary': summaries,\n",
    "            'keywords': keywords,\n",
    "            'embedding': embeddings\n",
    "        })\n",
    "\n",
    "        return df\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    # # Audio Processing\n",
    "\n",
    "    # def download_audio(self, url):\n",
    "    #     # Download audio from a given URL. This could be a conference call, interview, etc.\n",
    "    #     pass\n",
    "\n",
    "    # def trim_audio(self, audio, start_time, end_time):\n",
    "    #     # Trim the audio to only include content between the start and end times.\n",
    "    #     pass\n",
    "\n",
    "    # def transcribe_audio(self, audio):\n",
    "    #     # Convert the audio to text using a transcription service or API.\n",
    "    #     pass\n",
    "\n",
    "    # Text Processing\n",
    "    def process_transcription(self, transcription):\n",
    "        # Process the transcription to extract or generate structured data.\n",
    "        # This could involve parsing, tagging, or other NLP techniques.\n",
    "        pass\n",
    "\n",
    "    def summarize_transcription(self, transcription):\n",
    "        # Extract the actual transcription text from the dictionary\n",
    "        transcription_text = transcription.get('transcription', '')\n",
    "        \n",
    "        # Split the transcription into chunks of 3000 tokens each\n",
    "        tokens = transcription_text.split()\n",
    "        chunks = [' '.join(tokens[i:i + 3000]) for i in range(0, len(tokens), 3000)]\n",
    "\n",
    "        summaries = []\n",
    "        for chunk in chunks:\n",
    "            # Summarize each chunk using the OpenAI API\n",
    "            prompt = f\"Please provide a summary of the following text, a part of extracted text by a web page, max 500 words:\\n{chunk}\"\n",
    "            response = openai.Completion.create(\n",
    "                model=\"text-davinci-003\",\n",
    "                prompt=prompt,\n",
    "                max_tokens=500,\n",
    "                n=1,\n",
    "                stop=None,\n",
    "                temperature=0.5,\n",
    "            )\n",
    "            summaries.append(response.choices[0].text.strip())\n",
    "            \n",
    "\n",
    "        # Aggregate the summaries\n",
    "        aggregated_summary = ' '.join(summaries)\n",
    "\n",
    "        # Summarize the aggregated summary\n",
    "        prompt = f\"Please aggregate summaries of the following texts and then produce final detail summary max 500 words:\\n{aggregated_summary}\"\n",
    "        response = openai.Completion.create(\n",
    "            model=\"text-davinci-003\",\n",
    "            prompt=prompt,\n",
    "            max_tokens=500,\n",
    "            n=1,\n",
    "            stop=None,\n",
    "            temperature=0.5,\n",
    "        )\n",
    "        final_summary = response.choices[0].text.strip()\n",
    "\n",
    "        # Save the final summary to a text file\n",
    "        with open(f'final_summary_{self.youtube_video.title}.txt', 'w') as f:\n",
    "            f.write(final_summary)\n",
    "            \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import time\n",
    "import PyPDF2\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import ast # for string to list conversion\n",
    "# from scipy import spatial\n",
    "\n",
    "import openai\n",
    "# from openai.embeddings_utils import cosine_similarity\n",
    "\n",
    "# from weasyprint import HTML\n",
    "# from selenium import webdriver\n",
    "import fpdf\n",
    "import pdfkit\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "# import csv\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cs\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "# from reportlab.lib.pagesizes import letter\n",
    "# from reportlab.lib import colors\n",
    "# from reportlab.pdfgen import canvas\n",
    "\n",
    "# nltk.download('punkt')\n",
    "\n",
    "class AutomatedContentGenerator:\n",
    "    def __init__(self, api_key, cx, query, num_results, openai_api_key):\n",
    "        # Initialize any necessary variables, data structures, or connections.\n",
    "        # This could include a web scraper, data parser, text generator, etc.\n",
    "        self.api_key = api_key\n",
    "        self.cx = cx\n",
    "        self.query = query\n",
    "        self.num_results = num_results\n",
    "        self.results = []\n",
    "        openai.api_key = openai_api_key  # replace with your OpenAI API key\n",
    "        self.folder_path = f'{self.query}_output'\n",
    "        os.makedirs(self.folder_path, exist_ok=True)\n",
    "        self.visited_urls = set()\n",
    "\n",
    "    ## Step 1: Search Google\n",
    "    # Web Scraping and Parsing\n",
    "    def search_and_scrape_web(self):\n",
    "        # Use a web scraper to find and retrieve relevant information from the web based on the query.\n",
    "        # This could involve a search engine API or scraping specific sites.\n",
    "        url = 'https://www.googleapis.com/customsearch/v1'\n",
    "        params = {\n",
    "            'key': self.api_key,\n",
    "            'cx': self.cx,\n",
    "            'q': self.query,\n",
    "            'num': self.num_results,\n",
    "        }\n",
    "        response = requests.get(url, params=params)\n",
    "        data = response.json()\n",
    "        if 'items' in data:\n",
    "            for item in data['items']:\n",
    "                self.results.append({\n",
    "                    'title': item['title'],\n",
    "                    'link': item['link'],\n",
    "                    'snippet': item['snippet'],\n",
    "                    'date': item.get('pagemap', {}).get('metatags', [{}])[0].get('og:updated_time')\n",
    "                })\n",
    "    \n",
    "    def save_search_results_to_excel(self, filename=None):\n",
    "        if filename is None:\n",
    "            filename = f'{self.query}.xlsx'\n",
    "        df = pd.DataFrame(self.results)\n",
    "        df.to_excel(filename, index=False)\n",
    "        \n",
    "\n",
    "    ## Step 2: Scrape web pages and save them in pdf format\n",
    "    # Using Beautifulsoup and pdfkit\n",
    "    def extract_content(self, url):\n",
    "        # Send a GET request to the webpage\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        # Manually set the encoding to 'UTF-8'\n",
    "        response.encoding = 'UTF-8'\n",
    "\n",
    "        # Parse the HTML content of the webpage with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        \n",
    "        # Extract the title of the webpage\n",
    "        title = f\"Title:\\n{soup.h1}\" if soup.h1 else \"No Title\"\n",
    "\n",
    "        # Find the main article content by inspecting the HTML structure\n",
    "        article = soup.find('main')\n",
    "        if article is None:\n",
    "            possible_classes = ['main', 'main-content', 'article_main', 'container', 'head', 'headline', 'panel']\n",
    "            for class_name in possible_classes:\n",
    "                article = soup.find('main', class_= class_name)\n",
    "                if article is not None:\n",
    "                    break\n",
    "\n",
    "        # If the main article content still cannot be found, return an empty string\n",
    "        if article is None:\n",
    "            print(f\"Could not find main content in {url}\")\n",
    "            return \"\"\n",
    "\n",
    "        # Extract the text within h2, h3, h4, h5, h6, and p tags\n",
    "        headers_and_paragraphs = article.find_all(['h2', 'h3', 'h4', 'h5', 'h6', 'p'])\n",
    "\n",
    "        # Join the text from all tags into a single string with HTML tags\n",
    "        processed_main_content = ''.join(str(tag) for tag in headers_and_paragraphs)\n",
    "        \n",
    "        # Title and the content\n",
    "        processed_content = f\"<h1>{title}</h1>\" + processed_main_content\n",
    "        \n",
    "        # Limit the content to the first 3000 tokens  \n",
    "        content = self.limit_content_by_tokens(processed_content, 3000)\n",
    "\n",
    "        return content\n",
    "    \n",
    "    # limit the content by tokens instead of words\n",
    "    def limit_content_by_tokens(self, content, max_tokens):\n",
    "        tokens = content.split()\n",
    "        if len(tokens) > max_tokens:\n",
    "            tokens = tokens[:max_tokens]\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    \n",
    "    # # limit the content by tokens instead of words\n",
    "    # def limit_content_by_tokens(self, content, max_tokens):\n",
    "    #     tokens = []\n",
    "    #     token_count = 0\n",
    "\n",
    "    #     for word in content.split():\n",
    "    #         word_tokens = len(word.split()) + 1  # Add 1 for the space that follows each word\n",
    "    #         if token_count + word_tokens > max_tokens:\n",
    "    #             break\n",
    "    #         tokens.append(word)\n",
    "    #         token_count += word_tokens\n",
    "\n",
    "    #     return ' '.join(tokens)\n",
    "\n",
    "    # save the content to pdf\n",
    "    def save_content_to_pdf(self, url, content, filename):\n",
    "        # Add the title and the link at the beginning of the content\n",
    "        content_with_link = f\"{content}\\n\\n <h3>URL:</h3>\\n<p></p>\\n<p><a href='{url}'>{url}</a></p>\"\n",
    "\n",
    "        # # Split the content into words\n",
    "        # words = content_with_title_and_link.split()\n",
    "\n",
    "        # # Limit the content to the first 4000 words\n",
    "        # limited_content = ' '.join(words[:4000])\n",
    "\n",
    "        if not os.path.exists(self.folder_path):\n",
    "            os.makedirs(self.folder_path)\n",
    "        options = {\n",
    "            'no-stop-slow-scripts': True,\n",
    "            'load-error-handling': 'ignore',\n",
    "            'encoding': \"UTF-8\", ### This is important to solve quatation mark problem\n",
    "            \n",
    "        }\n",
    "        path_wkhtmltopdf = r'C:\\Program Files\\wkhtmltopdf\\bin\\wkhtmltopdf.exe'\n",
    "        config = pdfkit.configuration(wkhtmltopdf=path_wkhtmltopdf)\n",
    "        \n",
    "        # # Encode the HTML content as UTF-8 before passing it to pdfkit\n",
    "        # content_with_keywords_link = content_with_keywords_link.encode('utf-8')\n",
    "    \n",
    "        pdfkit.from_string(content_with_link, os.path.join(self.folder_path, filename), configuration=config, options=options) \n",
    "\n",
    "    # Loop through the URLs and scrape the content    \n",
    "    def scrape_and_save(self):\n",
    "        df = pd.read_excel(f'{self.query}.xlsx')\n",
    "           \n",
    "        for index, row in df.iterrows():\n",
    "            url = row['link']\n",
    "            # title = row['title']\n",
    "            if url not in self.visited_urls:\n",
    "                content = self.extract_content(url)\n",
    "                pdf_filename = f\"web_page_{index}.pdf\"\n",
    "                self.save_content_to_pdf(url, content, pdf_filename)\n",
    "                self.visited_urls.add(url)\n",
    "            else:\n",
    "                print(f\"Skipping already visited URL: {url}\")\n",
    "                \n",
    "            # Sleep for 1 second\n",
    "            time.sleep(1)\n",
    "                \n",
    "                \n",
    "    def extract_text_from_pdf(self, pdf_path):\n",
    "        #pdf_file_obj = open(pdf_path, 'rb')\n",
    "        pdf_reader = PdfReader(pdf_path)\n",
    "        pdf_content = ''\n",
    "        \n",
    "        # for page_num in range(pdf_reader.numPages):\n",
    "        #     page_obj = pdf_reader.getPage(page_num)\n",
    "        #     pdf_content += page_obj.extractText()\n",
    "        # pdf_file_obj.close()\n",
    "        for page in pdf_reader.pages:\n",
    "            pdf_content += page.extract_text()\n",
    "        \n",
    "        return pdf_content\n",
    "\n",
    "    def save_sentences_to_csv(self, pdf_content):\n",
    "        sentences = nltk.sent_tokenize(pdf_content)\n",
    "        df_sentences = pd.DataFrame(sentences, columns=['text'])\n",
    "        return df_sentences\n",
    "    \n",
    "    def get_embedding(self, text, model):\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "        if model == 'default_model':\n",
    "            # use default model to get embedding\n",
    "            pass\n",
    "        elif model == 'text-embedding-ada-002':\n",
    "            # use 'text-embedding-ada-002' model to get embedding\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model: {model}\")\n",
    "        return openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']\n",
    "\n",
    "\n",
    "\n",
    "    def get_embeddings(self, df_sentences):\n",
    "        df_sentences['embeddings'] = df_sentences['text'].apply(lambda x: self.get_embedding(x, model='text-embedding-ada-002'))\n",
    "        return df_sentences\n",
    "\n",
    "    # def get_similarities(self, df_sentences):\n",
    "    #     df_sentences['similarities'] = df_sentences['embeddings'].apply(lambda x: cosine_similarity(x, df_sentences['embeddings'].tolist()))\n",
    "    #     return df_sentences\n",
    "    \n",
    "    def get_similarities(self, df_sentences):\n",
    "        embeddings = np.stack(df_sentences['embeddings'].values)\n",
    "        similarities = cs(embeddings)\n",
    "        df_sentences['similarities'] = list(similarities)\n",
    "        return df_sentences\n",
    "\n",
    "    # # What algorithm should we use to cluster the sentences?\n",
    "    # def create_clustering_image(self, df_sentences):\n",
    "    #     matrix = np.vstack(df_sentences['embeddings'].values)\n",
    "    #     n_clusters = 3\n",
    "    #     kmeans = KMeans(n_clusters=n_clusters, init='k-means++', random_state=42)\n",
    "    #     kmeans.fit(matrix)\n",
    "    #     df_sentences['Cluster'] = kmeans.labels_\n",
    "        \n",
    "    #     # Ensure perplexity is less than the number of samples\n",
    "    #     perplexity = min(15, len(df_sentences) - 1)\n",
    "        \n",
    "    #     tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42, init=\"random\", learning_rate=200)\n",
    "    #     vis_dims2 = tsne.fit_transform(matrix)\n",
    "    #     x = [x for x, y in vis_dims2]\n",
    "    #     y = [y for x, y in vis_dims2]\n",
    "    #     fig, ax = plt.subplots()\n",
    "    #     for category, color in enumerate([\"purple\", \"green\", \"red\", \"blue\"]):\n",
    "    #         xs = np.array(x)[df_sentences.Cluster == category]\n",
    "    #         ys = np.array(y)[df_sentences.Cluster == category]\n",
    "    #         ax.scatter(xs, ys, color=color, alpha=0.3)\n",
    "    #         avg_x = xs.mean()\n",
    "    #         avg_y = ys.mean()\n",
    "    #         ax.scatter(avg_x, avg_y, marker=\"x\", color=color, s=100)\n",
    "    #     ax.set_title(\"Clusters identified visualized in language 2d using t-SNE\")\n",
    "    #     fig.canvas.draw()\n",
    "        \n",
    "    #     # Create a BytesIO object and save the figure to it in PNG format\n",
    "    #     buf = io.BytesIO()\n",
    "    #     fig.savefig(buf, format='png')\n",
    "    #     buf.seek(0)\n",
    "\n",
    "    #     # Close the figure\n",
    "    #     plt.close(fig)\n",
    "\n",
    "    #     return buf\n",
    "    \n",
    "    def create_clustering_image(self, df_sentences):\n",
    "        if len(df_sentences) < 2:\n",
    "            print(\"Not enough sentences to cluster\")\n",
    "            pass\n",
    "        else:\n",
    "            matrix = np.vstack(df_sentences['embeddings'].values)\n",
    "            n_clusters = 3\n",
    "            gmm = GaussianMixture(n_components=n_clusters, random_state=42)\n",
    "            gmm.fit(matrix)\n",
    "            df_sentences['Cluster'] = gmm.predict(matrix)\n",
    "            \n",
    "            # Ensure perplexity is less than the number of samples\n",
    "            perplexity = min(15, len(df_sentences) - 1)\n",
    "            \n",
    "            tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42, init=\"random\", learning_rate=200)\n",
    "            vis_dims2 = tsne.fit_transform(matrix)\n",
    "            x = [x for x, y in vis_dims2]\n",
    "            y = [y for x, y in vis_dims2]\n",
    "            fig, ax = plt.subplots()\n",
    "            for category, color in enumerate([\"purple\", \"green\", \"red\"]):\n",
    "                xs = np.array(x)[df_sentences.Cluster == category]\n",
    "                ys = np.array(y)[df_sentences.Cluster == category]\n",
    "                if len(xs) > 0 and len(ys) > 0:  # Check that the arrays are not empty\n",
    "                    ax.scatter(xs, ys, color=color, alpha=0.3)\n",
    "                    avg_x = xs.mean()\n",
    "                    avg_y = ys.mean()\n",
    "                    ax.scatter(avg_x, avg_y, marker=\"x\", color=color, s=100)\n",
    "            ax.set_title(\"Clusters identified visualized in language 2d using t-SNE\")\n",
    "            fig.canvas.draw()\n",
    "            \n",
    "            # Create a BytesIO object and save the figure to it in PNG format\n",
    "            buf = io.BytesIO()\n",
    "            fig.savefig(buf, format='png')\n",
    "            buf.seek(0)\n",
    "\n",
    "            # Close the figure\n",
    "            plt.close(fig)\n",
    "\n",
    "            return buf\n",
    "\n",
    "    # def add_image_to_pdf(self, pdf_path, image_buf):\n",
    "    #     pdf_files = [f for f in os.listdir(self.folder_path) if f.endswith('.pdf')]\n",
    "        \n",
    "    #     pass\n",
    "    \n",
    "    # def process_openai_embeddings_csv(self):\n",
    "    #     # Get a list of all PDF files in the folder\n",
    "    #     pdf_files = [f for f in os.listdir(self.folder_path) if f.endswith('.pdf')]\n",
    "    #     for pdf_file in pdf_files:\n",
    "    #         pdf_path = os.path.join(self.folder_path, pdf_file)\n",
    "    #         pdf_content = self.extract_text_from_pdf(pdf_path)\n",
    "    #         df_sentences = self.save_sentences_to_csv(pdf_content)\n",
    "    #         df_sentences = self.get_embeddings(df_sentences)\n",
    "    #         df_sentences = self.get_similarities(df_sentences)\n",
    "    #         df_sentences.to_csv(os.path.join(self.folder_path, f'{self.query}_openai.csv'), index=False)\n",
    "\n",
    "    #         # Sleep for 1 second\n",
    "    #         time.sleep(1)\n",
    "    \n",
    "    def process_pdfs_and_save_embeddings(self):\n",
    "        # Get a list of all PDF files in the folder\n",
    "        pdf_files = [f for f in os.listdir(self.folder_path) if f.endswith('.pdf')]\n",
    "        for pdf_file in pdf_files:\n",
    "            pdf_path = os.path.join(self.folder_path, pdf_file)\n",
    "            pdf_content = self.extract_text_from_pdf(pdf_path)\n",
    "            df_sentences = self.save_sentences_to_csv(pdf_content)\n",
    "            df_sentences = self.get_embeddings(df_sentences)\n",
    "            df_sentences = self.get_similarities(df_sentences)\n",
    "            df_sentences.to_csv(os.path.join(self.folder_path, f'{pdf_file}_embeddings.csv'), index=False)\n",
    "            \n",
    "            # Sleep for 1 second\n",
    "            time.sleep(1)\n",
    "            \n",
    "    \n",
    "    def create_clustering_images_from_saved_embeddings(self):\n",
    "        # Get a list of all CSV files in the folder\n",
    "        csv_files = [f for f in os.listdir(self.folder_path) if f.endswith('_embeddings.csv')]\n",
    "        for csv_file in csv_files:\n",
    "            csv_path = os.path.join(self.folder_path, csv_file)\n",
    "            df_sentences = pd.read_csv(csv_path)\n",
    "            \n",
    "            # Convert the embeddings from strings back to lists of floats\n",
    "            df_sentences['embeddings'] = df_sentences['embeddings'].apply(ast.literal_eval).apply(lambda x: [float(i) for i in x])\n",
    "            \n",
    "            # Create the clustering image\n",
    "            image_buf = self.create_clustering_image(df_sentences)\n",
    "            \n",
    "            if image_buf is not None:\n",
    "                # Convert the image buffer to a PIL image\n",
    "                image = Image.open(image_buf)\n",
    "                \n",
    "                # Convert image to RGB if it's RGBA\n",
    "                if image.mode == 'RGBA':\n",
    "                    image = image.convert('RGB')\n",
    "                \n",
    "                # Save the image as a JPEG file\n",
    "                image.save(os.path.join(self.folder_path, f'{csv_file}_cluster.jpeg'))\n",
    "                \n",
    "                # Sleep for 1 second\n",
    "                time.sleep(1)\n",
    "            \n",
    "    # def execute_create_clustering_image(self):\n",
    "    #     # Load the sentences from the CSV file\n",
    "    #     df_sentences = pd.read_csv(os.path.join(self.folder_path, f'{self.query}_openai.csv'))\n",
    "        \n",
    "    #     # Convert the embeddings from strings back to lists of floats\n",
    "    #     df_sentences['embeddings'] = df_sentences['embeddings'].apply(ast.literal_eval).apply(lambda x: [float(i) for i in x])\n",
    "        \n",
    "    #     # Create the clustering image\n",
    "    #     image_buf = self.create_clustering_image(df_sentences)\n",
    "        \n",
    "    #     # Convert the image buffer to a PIL image\n",
    "    #     image = Image.open(image_buf)\n",
    "            \n",
    "    #     # Convert image to RGB if it's RGBA\n",
    "    #     if image.mode == 'RGBA':\n",
    "    #         image = image.convert('RGB')\n",
    "            \n",
    "    #     # Save the image as a JPEG file\n",
    "    #     image.save(os.path.join(self.folder_path, f'{self.query}_clustering.jpeg'))\n",
    "\n",
    "                        \n",
    "    # def execute_create_clustering_image(self):\n",
    "    #     # Load the sentences from the CSV file\n",
    "    #     df_sentences = pd.read_csv(os.path.join(self.folder_path, f'{self.query}_openai.csv'))\n",
    "        \n",
    "    #     # Create the clustering image\n",
    "    #     image_buf = self.create_clustering_image(df_sentences)\n",
    "        \n",
    "    #     # Convert the image buffer to a PIL image\n",
    "    #     image = Image.open(image_buf)\n",
    "            \n",
    "    #     # Convert image to RGB if it's RGBA\n",
    "    #     if image.mode == 'RGBA':\n",
    "    #         image = image.convert('RGB')\n",
    "            \n",
    "    #     # Save the image as a JPEG file\n",
    "    #     image.save(os.path.join(self.folder_path, f'{self.query}_clustering.jpeg'))\n",
    "        \n",
    "        \n",
    "    # def process_saved_pdfs(self):\n",
    "    #     # Get a list of all PDF files in the folder\n",
    "    #     pdf_files = [f for f in os.listdir(self.folder_path) if f.endswith('.pdf')]\n",
    "\n",
    "    #     for pdf_file in pdf_files:\n",
    "    #         # Extract text from the PDF file\n",
    "    #         pdf_content = self.extract_text_from_pdf(os.path.join(self.folder_path, pdf_file))\n",
    "\n",
    "    #         # Split the content into sentences\n",
    "    #         sentences = nltk.sent_tokenize(pdf_content)\n",
    "\n",
    "    #         # Get embeddings for each sentence\n",
    "    #         embeddings = [self.get_embedding(sentence) for sentence in sentences]\n",
    "\n",
    "    #         # Create a DataFrame with the sentences and their embeddings\n",
    "    #         df_sentences = pd.DataFrame({\n",
    "    #             'index': range(len(sentences)),\n",
    "    #             'text': sentences,\n",
    "    #             'embeddings': embeddings\n",
    "    #         })\n",
    "\n",
    "    #         # Calculate similarities between the sentences\n",
    "    #         df_sentences['similarities'] = df_sentences['embeddings'].apply(lambda x: cosine_similarity(x, embeddings))\n",
    "\n",
    "    #         # Save the DataFrame to a CSV file\n",
    "    #         df_sentences.to_csv(os.path.join(self.folder_path, f'{self.query}_openai.csv'), index=False)\n",
    "            \n",
    "    #         keywords = self.extract_keywords(pdf_content)\n",
    "    #         summary = self.summarize_text(pdf_content)\n",
    "    #         self.save_summary_and_keywords_to_pdf(url, row['title'], keywords, summary)\n",
    "\n",
    "\n",
    "    \n",
    "    # def get_embedding(self, pdf_content, filename):\n",
    "        \n",
    "    #     if not os.path.exists(f'{self.query}_pdfs'):\n",
    "    #         os.makedirs(f'{self.query}_pdfs')\n",
    "        \n",
    "    #     # Split the content into sentences\n",
    "    #     sentences = nltk.sent_tokenize(pdf_content)\n",
    "    #     embeddings = []\n",
    "    #     for sentence in sentences:\n",
    "    #         sentence = sentence.replace(\"\\n\", \" \")\n",
    "    #         result = openai.Embedding.create(input=[sentence], model=\"text-embedding-ada-002\")['data'][0]['embedding']\n",
    "    #         embeddings.append(result)\n",
    "    #     df = pd.DataFrame(embeddings)\n",
    "    #     df['embedding'] = df.combined.apply(lambda x: get_embedding(x, model='text-embedding-ada-002'))\n",
    "    #     df.to_excel(filename, index=False)\n",
    "        \n",
    "    #            \n",
    "    def extract_keywords(self, pdf_content):\n",
    "        response = openai.Completion.create(\n",
    "            model=\"text-davinci-003\",\n",
    "            prompt=f\"Extract main 10 keywords from the following text as #Keywords form:\\n\\n{pdf_content}\\n\\n tl;dr:\",\n",
    "            temperature=0.5,\n",
    "            max_tokens=200,\n",
    "            top_p=1.0,\n",
    "            frequency_penalty=0.8,\n",
    "            presence_penalty=0.0\n",
    "        )\n",
    "        keywords = response.choices[0].text.strip().split(', ')\n",
    "        return keywords         \n",
    "    \n",
    "    def summarize_text(self, pdf_content):\n",
    "        # Split the text into chunks of 1000 tokens each\n",
    "        tokens = pdf_content.split()\n",
    "        chunks = [' '.join(tokens[i:i + 2000]) for i in range(0, len(tokens), 2000)]\n",
    "\n",
    "        summaries = []\n",
    "        for chunk in chunks:\n",
    "            # Summarize each chunk using the OpenAI API\n",
    "            prompt = f\"Please provide a detailed summary of the following text with keywords max 500 words:\\n{chunk}\\n\\n tl;dr:\"\n",
    "            response = openai.Completion.create(\n",
    "                model=\"text-davinci-003\",\n",
    "                prompt=prompt,\n",
    "                max_tokens=2000,\n",
    "                n=1,\n",
    "                stop=None,\n",
    "                temperature=0.5,\n",
    "            )\n",
    "            summaries.append(response.choices[0].text.strip())\n",
    "\n",
    "        # Aggregate the summaries\n",
    "        aggregated_summary = ' '.join(summaries)\n",
    "\n",
    "        return aggregated_summary\n",
    "           \n",
    "    # def summarize_text(self, pdf_content):\n",
    "    #     # Split the text into chunks of 2000 tokens each\n",
    "    #     tokens = pdf_content.split()\n",
    "    #     # chunks = [' '.join(tokens[i:i + 1000]) for i in range(0, len(tokens), 1000)]\n",
    "\n",
    "    #     # summaries = []\n",
    "    #     # for chunk in chunks:\n",
    "    #     #     # Summarize each chunk using the OpenAI API\n",
    "    #     #     prompt = f\"Please provide a bit detailed summary of the following text with keywords max 300 words:\\n{chunk}\\n\\n tl;dr:\"\n",
    "    #     #     response = openai.Completion.create(\n",
    "    #     #         model=\"text-davinci-003\",\n",
    "    #     #         prompt=prompt,\n",
    "    #     #         max_tokens=1000,\n",
    "    #     #         n=1,\n",
    "    #     #         stop=None,\n",
    "    #     #         temperature=0.5,\n",
    "    #     #     )\n",
    "    #     #     summaries.append(response.choices[0].text.strip())\n",
    "\n",
    "    #     # # Aggregate the summaries\n",
    "    #     # aggregated_summary = ' '.join(summaries)\n",
    "\n",
    "    #     # Summarize the aggregated summary\n",
    "    #     prompt = f\"The following text is scrapped text of a webpage. Summarize the text and produce an summary ariticle like Bloomberg or Harvard Business Review, max 600 words with core keywords:\\n{tokens}\\n\\n tl;dr:\"\n",
    "    #     response = openai.Completion.create(\n",
    "    #         model=\"text-davinci-003\",\n",
    "    #         prompt=prompt,\n",
    "    #         max_tokens=3000,\n",
    "    #         n=1,\n",
    "    #         stop=None,\n",
    "    #         temperature=0.5,\n",
    "    #     )\n",
    "    #     summary = response.choices[0].text.strip()\n",
    "\n",
    "    #     return summary\n",
    "    \n",
    "    # def save_summary_and_keywords_to_pdf(self, url, title, keywords, summary):\n",
    "    #     content = f\"<h1>{title}</h1>\\n\\n<h2>Keywords:</h2>\\n<p>{', '.join(keywords)}</p>\\n\\n<h2>Summary:</h2>\\n<p>{summary}</p>\\n\\n<h3>URL:</h3>\\n<p><a href='{url}'>{url}</a></p>\"\n",
    "    #     if not os.path.exists(self.folder_path):\n",
    "    #         os.makedirs(self.folder_path)\n",
    "    #     options = {\n",
    "    #         'no-stop-slow-scripts': True,\n",
    "    #         'load-error-handling': 'ignore',\n",
    "    #         'encoding': \"UTF-8\",\n",
    "    #     }\n",
    "    #     path_wkhtmltopdf = r'C:\\Program Files\\wkhtmltopdf\\bin\\wkhtmltopdf.exe'\n",
    "    #     config = pdfkit.configuration(wkhtmltopdf=path_wkhtmltopdf)\n",
    "    #     pdfkit.from_string(content, os.path.join(self.folder_path, 'output.pdf'), configuration=config, options=options) \n",
    "\n",
    "\n",
    "    def save_summary_and_keywords_to_pdf(self, url, title, keywords, summary):\n",
    "        content = f\"<h1>{title}</h1>\\n\\n<h2>Keywords:</h2>\\n<p>{', '.join(keywords)}</p>\\n\\n<h2>Summary:</h2>\\n<p>{summary}</p>\\n\\n<h3>URL:</h3>\\n<p><a href='{url}'>{url}</a></p>\"\n",
    "        if not os.path.exists(self.folder_path):\n",
    "            os.makedirs(self.folder_path)\n",
    "        options = {\n",
    "            'no-stop-slow-scripts': True,\n",
    "            'load-error-handling': 'ignore',\n",
    "            'encoding': \"UTF-8\",\n",
    "        }\n",
    "        path_wkhtmltopdf = r'C:\\Program Files\\wkhtmltopdf\\bin\\wkhtmltopdf.exe'\n",
    "        config = pdfkit.configuration(wkhtmltopdf=path_wkhtmltopdf)\n",
    "        pdfkit.from_string(content, os.path.join(self.folder_path, 'output.pdf'), configuration=config, options=options)\n",
    "\n",
    "        # If the summary PDF already exists, append the new summary to it\n",
    "        pdf_filename = f\"{self.query}_summary.pdf\"\n",
    "        pdf_path = os.path.join(self.folder_path, pdf_filename)\n",
    "        if os.path.exists(pdf_path):\n",
    "            writer = PdfWriter()\n",
    "\n",
    "            # Existing PDF\n",
    "            existing_pdf = PdfReader(open(pdf_path, \"rb\"))\n",
    "            for page in existing_pdf.pages:\n",
    "                writer.add_page(page)\n",
    "\n",
    "            # New PDF\n",
    "            new_pdf = PdfReader(open(os.path.join(self.folder_path, 'output.pdf'), \"rb\"))\n",
    "            writer.add_page(new_pdf.pages[0])\n",
    "\n",
    "            # Write the output PDF\n",
    "            with open(pdf_path, \"wb\") as f:\n",
    "                writer.write(f)\n",
    "        else:\n",
    "            # If the summary PDF doesn't exist, create a new one\n",
    "            os.rename(os.path.join(self.folder_path, 'output.pdf'), pdf_path)\n",
    "    \n",
    "\n",
    "    def process_saved_pdfs(self):\n",
    "        df = pd.read_excel(f'{self.query}.xlsx')\n",
    "        for index, row in df.iterrows():\n",
    "            url = row['link']\n",
    "            pdf_filename = f\"web_page_{index}.pdf\"\n",
    "            pdf_path = os.path.join(self.folder_path, pdf_filename)\n",
    "            pdf_content = self.extract_text_from_pdf(pdf_path)\n",
    "            keywords = self.extract_keywords(pdf_content)\n",
    "\n",
    "            # Split the PDF content into chunks of 2000 tokens each\n",
    "            tokens = pdf_content.split()\n",
    "            chunks = [' '.join(tokens[i:i + 2000]) for i in range(0, len(tokens), 2000)]\n",
    "\n",
    "            # Summarize each chunk separately\n",
    "            summaries = []\n",
    "            for chunk in chunks:\n",
    "                summary = self.summarize_text(chunk)\n",
    "                summaries.append(summary)\n",
    "\n",
    "            # Aggregate the summaries\n",
    "            aggregated_summary = ' '.join(summaries)\n",
    "            \n",
    "            final_summary = self.summarize_text(aggregated_summary)\n",
    "            \n",
    "            # Extract the title from the PDF content\n",
    "            # title_start = pdf_content.find('Title:') + len('Title:')\n",
    "            # title_end = pdf_content.find('\\n', title_start)\n",
    "            # title = pdf_content[title_start:title_end].strip()\n",
    "\n",
    "            # Save the summary and keywords to a PDF\n",
    "            self.save_summary_and_keywords_to_pdf(url, row['title'], keywords, final_summary)\n",
    "\n",
    "            # Sleep for 1 second\n",
    "            time.sleep(1)\n",
    "            \n",
    "    def generate_article(self):\n",
    "        pdf_filename = f\"{self.query}_summary.pdf\"\n",
    "        pdf_path = os.path.join(self.folder_path, pdf_filename)\n",
    "        # news_title = input(\"Please enter the title of the news: \")\n",
    "        \n",
    "        # Get the current date\n",
    "        today = datetime.now().date()\n",
    "        \n",
    "        if os.path.exists(pdf_path):\n",
    "            pdf_content = self.extract_text_from_pdf(pdf_path)\n",
    "            \n",
    "            # # Split the text into chunks of 2000 tokens each\n",
    "            # tokens = pdf_content.split()\n",
    "            # chunks = [' '.join(tokens[i:i + 2000]) for i in range(0, len(tokens), 2000)]\n",
    "            \n",
    "            # Produce an professional and detailed journalism article like Harvard Business Review by using resources below, around 2000 tokens length. Also do paragraphing and extract core keywords from the article created.\n",
    "            prompt = f\"The following text is scrapped text from multiple webpages as the results of {self.query}. \\n Produce a total professional summary from the text below around 1000 words:\\n{pdf_content}\\n\"\n",
    "            response = openai.Completion.create(\n",
    "                    model=\"text-davinci-003\",\n",
    "                    prompt=prompt,\n",
    "                    max_tokens=2000,\n",
    "                    n=1,\n",
    "                    stop=None,\n",
    "                    temperature=0.5,\n",
    "                )\n",
    "\n",
    "            # summaries = []\n",
    "            # for chunk in chunks:\n",
    "            #     # Summarize each chunk using the OpenAI API\n",
    "            #     prompt = f\"The following text is scrapped text of a webpage. Summarize the text and produce an summary article like Bloomberg or Harvard Business Review, max 600 words with core keywords:\\n{chunk}\\n\\n tl;dr:\"\n",
    "            #     response = openai.Completion.create(\n",
    "            #         model=\"text-davinci-003\",\n",
    "            #         prompt=prompt,\n",
    "            #         max_tokens=1000,\n",
    "            #         n=1,\n",
    "            #         stop=None,\n",
    "            #         temperature=0.5,\n",
    "            #     )\n",
    "                # summaries.append(response.choices[0].text.strip())\n",
    "\n",
    "            # # Aggregate the summaries\n",
    "            # generated_article = ' '.join(summaries)\n",
    "            \n",
    "            generated_article = response.choices[0].text.strip()\n",
    "            \n",
    "            news_article = f\"<h1>Title: The Total Summary of {self.query} </h1> \\n <h4> Date: {today} </h4> \\n\\n <p>{generated_article}<p>\"\n",
    "            \n",
    "            options = {\n",
    "                'no-stop-slow-scripts': True,\n",
    "                'load-error-handling': 'ignore',\n",
    "                'encoding': \"UTF-8\",\n",
    "            }\n",
    "            path_wkhtmltopdf = r'C:\\Program Files\\wkhtmltopdf\\bin\\wkhtmltopdf.exe'\n",
    "            config = pdfkit.configuration(wkhtmltopdf=path_wkhtmltopdf)\n",
    "            pdfkit.from_string(news_article, os.path.join(self.folder_path, f\"{self.query}_final.pdf\"), configuration=config, options=options)\n",
    "\n",
    "\n",
    "    # def process_saved_pdfs(self):\n",
    "    #     df = pd.read_excel(f'{self.query}.xlsx')\n",
    "    #     for index, row in df.iterrows():\n",
    "    #         url = row['link']\n",
    "    #         pdf_filename = f\"web_page_{index}.pdf\"\n",
    "    #         pdf_path = os.path.join(self.folder_path, pdf_filename)\n",
    "    #         pdf_content = self.extract_text_from_pdf(pdf_path)\n",
    "    #         keywords = self.extract_keywords(pdf_content)\n",
    "    #         summary = self.summarize_text(pdf_content)\n",
    "\n",
    "    #         # Create a PDF object\n",
    "    #         pdf = fpdf()\n",
    "\n",
    "    #         # Add a page\n",
    "    #         pdf.add_page()\n",
    "\n",
    "    #         # Set font\n",
    "    #         pdf.set_font(\"Arial\", size = 12)\n",
    "\n",
    "    #         # Add a cell\n",
    "    #         pdf.cell(200, 10, txt = f\"URL: {url}\", ln = True, align = 'L')\n",
    "    #         pdf.cell(200, 10, txt = f\"Title: {title}\", ln = True, align = 'L')\n",
    "    #         pdf.cell(200, 10, txt = f\"Keywords: {', '.join(keywords)}\", ln = True, align = 'L')\n",
    "    #         pdf.cell(200, 10, txt = f\"Summary: {summary}\", ln = True, align = 'L')\n",
    "\n",
    "    #         # Save the pdf with name .pdf\n",
    "    #         pdf_filename = f\"{self.query}_summary.pdf\"\n",
    "    #         pdf_path = os.path.join(self.folder_path, pdf_filename)\n",
    "            \n",
    "    #         if os.path.exists(pdf_path):\n",
    "    #             # If the summary PDF already exists, append the new summary to it\n",
    "    #             writer = PdfFileWriter()\n",
    "\n",
    "    #             # Existing PDF\n",
    "    #             existing_pdf = PdfFileReader(open(pdf_path, \"rb\"))\n",
    "    #             writer.addPage(existing_pdf.getPage(0))\n",
    "\n",
    "    #             # New PDF\n",
    "    #             new_pdf = PdfFileReader(pdf)\n",
    "    #             writer.addPage(new_pdf.getPage(0))\n",
    "\n",
    "    #             with open(pdf_path, \"wb\") as f:\n",
    "    #                 writer.write(f)\n",
    "    #         else:\n",
    "    #             # If the summary PDF doesn't exist, create a new one\n",
    "    #             pdf.output(pdf_path)\n",
    "                \n",
    "    #         # Sleep for 1 second\n",
    "    #         time.sleep(1)\n",
    "    \n",
    "    # def process_saved_pdfs(self):\n",
    "    #     df = pd.read_excel(f'{self.query}.xlsx')\n",
    "    #     for index, row in df.iterrows():\n",
    "    #         url = row['link']\n",
    "    #         pdf_filename = f\"web_page_{index}.pdf\"\n",
    "    #         pdf_path = os.path.join(self.folder_path, pdf_filename)\n",
    "    #         pdf_content = self.extract_text_from_pdf(pdf_path)\n",
    "    #         keywords = self.extract_keywords(pdf_content)\n",
    "    #         summary = self.summarize_text(pdf_content)\n",
    "    #         self.save_summary_and_keywords_to_pdf(url, row['title'], keywords, summary)\n",
    "            \n",
    "    #         # Sleep for 1 second\n",
    "    #         time.sleep(1)\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "    def handle_structured_data(self, data):\n",
    "        # Process the scraped data to extract or generate structured data.\n",
    "        # This could include parsing HTML, handling databases or spreadsheets, etc.\n",
    "        pass\n",
    "\n",
    "    # Content Generation\n",
    "\n",
    "    def personalize_content(self, user_profile, content):\n",
    "        # Adjust the content based on the user's profile or preferences.\n",
    "        # This could involve filtering, sorting, or generating new content.\n",
    "        pass\n",
    "\n",
    "    def fact_check(self, content):\n",
    "        # Verify the accuracy of the content. This could involve cross-referencing sources,\n",
    "        # using fact-checking APIs, or similar techniques.\n",
    "        pass\n",
    "\n",
    "    def produce_article(self, structured_data):\n",
    "        # Generate a coherent article or report from the structured data.\n",
    "        # This could involve natural language generation techniques.\n",
    "        pass\n",
    "\n",
    "    # Testing and Feedback\n",
    "\n",
    "    def test_article(self, article):\n",
    "        # Test the generated article for readability, accuracy, relevance, etc.\n",
    "        # This could involve NLP metrics, user feedback, or other tests.\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the AutomatedContentGenerator class\n",
    "acg = AutomatedContentGenerator(api_key='AIzaSyCIGuLmSrM65sXknalTE4B4x8PsCZpAZ-I', cx='d4c72f123415549d9', query='chatgpt use cases 2023 news', num_results=5, openai_api_key='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')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "acg.search_and_scrape_web()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "acg.save_search_results_to_excel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find main content in https://research.aimultiple.com/chatgpt-use-cases/\n",
      "Could not find main content in https://www.bloomberg.com/news/articles/2023-04-18/ai-therapy-becomes-new-use-case-for-chatgpt\n"
     ]
    }
   ],
   "source": [
    "acg.scrape_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jaden\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "acg.process_pdfs_and_save_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough sentences to cluster\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\jaden\\OneDrive\\Coding\\Program\\Python 3.11\\Lib\\site-packages\\PIL\\Image.py:3231\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3230\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3231\u001b[0m     fp\u001b[39m.\u001b[39;49mseek(\u001b[39m0\u001b[39m)\n\u001b[0;32m   3232\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mAttributeError\u001b[39;00m, io\u001b[39m.\u001b[39mUnsupportedOperation):\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'seek'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m acg\u001b[39m.\u001b[39;49mcreate_clustering_images_from_saved_embeddings()\n",
      "Cell \u001b[1;32mIn[24], line 366\u001b[0m, in \u001b[0;36mAutomatedContentGenerator.create_clustering_images_from_saved_embeddings\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    363\u001b[0m image_buf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_clustering_image(df_sentences)\n\u001b[0;32m    365\u001b[0m \u001b[39m# Convert the image buffer to a PIL image\u001b[39;00m\n\u001b[1;32m--> 366\u001b[0m image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mopen(image_buf)\n\u001b[0;32m    368\u001b[0m \u001b[39m# Convert image to RGB if it's RGBA\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[39mif\u001b[39;00m image\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mRGBA\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\jaden\\OneDrive\\Coding\\Program\\Python 3.11\\Lib\\site-packages\\PIL\\Image.py:3233\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3231\u001b[0m     fp\u001b[39m.\u001b[39mseek(\u001b[39m0\u001b[39m)\n\u001b[0;32m   3232\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mAttributeError\u001b[39;00m, io\u001b[39m.\u001b[39mUnsupportedOperation):\n\u001b[1;32m-> 3233\u001b[0m     fp \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mBytesIO(fp\u001b[39m.\u001b[39;49mread())\n\u001b[0;32m   3234\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   3236\u001b[0m prefix \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39mread(\u001b[39m16\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "acg.create_clustering_images_from_saved_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "acg.process_saved_pdfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "acg.generate_article()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('Title:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DocumentInformation',\n",
       " 'PageObject',\n",
       " 'PageRange',\n",
       " 'PaperSize',\n",
       " 'PasswordType',\n",
       " 'PdfFileMerger',\n",
       " 'PdfFileReader',\n",
       " 'PdfFileWriter',\n",
       " 'PdfMerger',\n",
       " 'PdfReader',\n",
       " 'PdfWriter',\n",
       " 'Transformation',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " '__warningregistry__',\n",
       " '_cmap',\n",
       " '_codecs',\n",
       " '_encryption',\n",
       " '_merger',\n",
       " '_page',\n",
       " '_protocols',\n",
       " '_reader',\n",
       " '_security',\n",
       " '_utils',\n",
       " '_version',\n",
       " '_writer',\n",
       " 'constants',\n",
       " 'errors',\n",
       " 'filters',\n",
       " 'generic',\n",
       " 'pagerange',\n",
       " 'papersizes',\n",
       " 'parse_filename_page_ranges',\n",
       " 'types',\n",
       " 'warnings',\n",
       " 'xmp']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(PyPDF2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text/html; charset=UTF-8\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://www.salesforce.com/news/press-releases/2023/03/07/einstein-generative-ai/'\n",
    "response = requests.get(url)\n",
    "\n",
    "print(response.headers['Content-Type'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"word\"\n",
    "len(word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method AutomatedContentGenerator.scrape_summarize_and_save_beta of <__main__.AutomatedContentGenerator object at 0x0000029DF1D0A810>>\n"
     ]
    }
   ],
   "source": [
    "print(acg.scrape_summarize_and_save_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "This model's maximum context length is 4097 tokens, however you requested 4418 tokens (3818 in your prompt; 600 for the completion). Please reduce your prompt; or completion length.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Scrape web pages and save them as PDFs\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m acg\u001b[39m.\u001b[39;49mscrape_summarize_and_save()\n",
      "Cell \u001b[1;32mIn[23], line 286\u001b[0m, in \u001b[0;36mAutomatedContentGenerator.scrape_summarize_and_save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mif\u001b[39;00m url \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvisited_urls:\n\u001b[0;32m    285\u001b[0m     content \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mextract_content(url)\n\u001b[1;32m--> 286\u001b[0m     summarized_content \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msummarize_text(content)\n\u001b[0;32m    287\u001b[0m     keywords \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mextract_keywords(summarized_content)\n\u001b[0;32m    288\u001b[0m     structured_content \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mArticle: \u001b[39m\u001b[39m{\u001b[39;00mcontent\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m <h2>Keywords:</h2> \u001b[39m\u001b[39m{\u001b[39;00mkeywords\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m <h2>Summarized:</h2> \u001b[39m\u001b[39m{\u001b[39;00msummarized_content\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[23], line 227\u001b[0m, in \u001b[0;36mAutomatedContentGenerator.summarize_text\u001b[1;34m(self, content)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m chunks:\n\u001b[0;32m    225\u001b[0m     \u001b[39m# Summarize each chunk using the OpenAI API\u001b[39;00m\n\u001b[0;32m    226\u001b[0m     prompt \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPlease provide a bit detailed summary of the following text with keywords max 500 words:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mchunk\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 227\u001b[0m     response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[0;32m    228\u001b[0m         model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtext-davinci-003\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    229\u001b[0m         prompt\u001b[39m=\u001b[39;49mprompt,\n\u001b[0;32m    230\u001b[0m         max_tokens\u001b[39m=\u001b[39;49m\u001b[39m600\u001b[39;49m,\n\u001b[0;32m    231\u001b[0m         n\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m    232\u001b[0m         stop\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    233\u001b[0m         temperature\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m,\n\u001b[0;32m    234\u001b[0m     )\n\u001b[0;32m    235\u001b[0m     summaries\u001b[39m.\u001b[39mappend(response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mstrip())\n\u001b[0;32m    237\u001b[0m \u001b[39m# Aggregate the summaries\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jaden\\OneDrive\\Coding\\Program\\Python 3.11\\Lib\\site-packages\\openai\\api_resources\\completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[1;32mc:\\Users\\jaden\\OneDrive\\Coding\\Program\\Python 3.11\\Lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    155\u001b[0m         url,\n\u001b[0;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32mc:\\Users\\jaden\\OneDrive\\Coding\\Program\\Python 3.11\\Lib\\site-packages\\openai\\api_requestor.py:226\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    206\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    207\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    214\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    215\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m    216\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[0;32m    217\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[0;32m    218\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    224\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[0;32m    225\u001b[0m     )\n\u001b[1;32m--> 226\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[0;32m    227\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[1;32mc:\\Users\\jaden\\OneDrive\\Coding\\Program\\Python 3.11\\Lib\\site-packages\\openai\\api_requestor.py:619\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    611\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    612\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    613\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    614\u001b[0m         )\n\u001b[0;32m    615\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[0;32m    616\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    617\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    618\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 619\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[0;32m    620\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    621\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[0;32m    622\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    623\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    624\u001b[0m         ),\n\u001b[0;32m    625\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    626\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\jaden\\OneDrive\\Coding\\Program\\Python 3.11\\Lib\\site-packages\\openai\\api_requestor.py:682\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    680\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[0;32m    681\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[1;32m--> 682\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[0;32m    683\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mInvalidRequestError\u001b[0m: This model's maximum context length is 4097 tokens, however you requested 4418 tokens (3818 in your prompt; 600 for the completion). Please reduce your prompt; or completion length."
     ]
    }
   ],
   "source": [
    "# Scrape web pages and save them as PDFs\n",
    "acg.scrape_summarize_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from the scraped PDFs\n",
    "pdf_texts = acg.extract_text_from_pdfs_in_folder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "Timeout",
     "evalue": "Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\jaden\\OneDrive\\Coding\\Program\\Python 3.11\\Lib\\site-packages\\urllib3\\connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    445\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    446\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    448\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m--> 449\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\jaden\\OneDrive\\Coding\\Program\\Python 3.11\\Lib\\site-packages\\urllib3\\connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 444\u001b[0m     httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[0;32m    445\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    446\u001b[0m     \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m     \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    448\u001b[0m     \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jaden\\OneDrive\\Coding\\Program\\Python 3.11\\Lib\\http\\client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1373\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1374\u001b[0m     response\u001b[39m.\u001b[39;49mbegin()\n\u001b[0;32m   1375\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jaden\\OneDrive\\Coding\\Program\\Python 3.11\\Lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[0;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n",
      "File \u001b[1;32mc:\\Users\\jaden\\OneDrive\\Coding\\Program\\Python 3.11\\Lib\\http\\client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mreadline(_MAXLINE \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n",
      "File \u001b[1;32mc:\\Users\\jaden\\OneDrive\\Coding\\Program\\Python 3.11\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    707\u001b[0m \u001b[39mexcept\u001b[39;00m timeout:\n",
      "File \u001b[1;32mc:\\Users\\jaden\\OneDrive\\Coding\\Program\\Python 3.11\\Lib\\ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1275\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1276\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1277\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1278\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1279\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jaden\\OneDrive\\Coding\\Program\\Python 3.11\\Lib\\ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1133\u001b[0m \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1134\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1135\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mTimeoutError\u001b[0m: The read operation timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\jaden\\OneDrive\\Coding\\Program\\Python 3.11\\Lib\\site-packages\\requests\\adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[1;32m--> 489\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    490\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[0;32m    491\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    492\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[0;32m    493\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    494\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    495\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    496\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    497\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    498\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    499\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    500\u001b[0m     )\n\u001b[0;32m    502\u001b[0m \u001b[39m# Send the request.\u001b[39;00m\n\u001b[0;32m    503\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jaden\\OneDrive\\Coding\\Program\\Python 3.11\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    785\u001b[0m     e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, e)\n\u001b[1;32m--> 787\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[0;32m    788\u001b[0m     method, url, error\u001b[39m=\u001b[39;49me, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[0;32m    789\u001b[0m )\n\u001b[0;32m    790\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
      "File \u001b[1;32mc:\\Users\\jaden\\OneDrive\\Coding\\Program\\Python 3.11\\Lib\\site-packages\\urllib3\\util\\retry.py:550\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    549\u001b[0m \u001b[39mif\u001b[39;00m read \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_method_retryable(method):\n\u001b[1;32m--> 550\u001b[0m     \u001b[39mraise\u001b[39;00m six\u001b[39m.\u001b[39;49mreraise(\u001b[39mtype\u001b[39;49m(error), error, _stacktrace)\n\u001b[0;32m    551\u001b[0m \u001b[39melif\u001b[39;00m read \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jaden\\OneDrive\\Coding\\Program\\Python 3.11\\Lib\\site-packages\\urllib3\\packages\\six.py:770\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[39mraise\u001b[39;00m value\u001b[39m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m--> 770\u001b[0m     \u001b[39mraise\u001b[39;00m value\n\u001b[0;32m    771\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jaden\\OneDrive\\Coding\\Program\\Python 3.11\\Lib\\site-packages\\urllib3\\connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[0;32m    704\u001b[0m     conn,\n\u001b[0;32m    705\u001b[0m     method,\n\u001b[0;32m    706\u001b[0m     url,\n\u001b[0;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[0;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    711\u001b[0m )\n\u001b[0;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jaden\\OneDrive\\Coding\\Program\\Python 3.11\\Lib\\site-packages\\urllib3\\connectionpool.py:451\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 451\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_timeout(err\u001b[39m=\u001b[39;49me, url\u001b[39m=\u001b[39;49murl, timeout_value\u001b[39m=\u001b[39;49mread_timeout)\n\u001b[0;32m    452\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jaden\\OneDrive\\Coding\\Program\\Python 3.11\\Lib\\site-packages\\urllib3\\connectionpool.py:340\u001b[0m, in \u001b[0;36mHTTPConnectionPool._raise_timeout\u001b[1;34m(self, err, url, timeout_value)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(err, SocketTimeout):\n\u001b[1;32m--> 340\u001b[0m     \u001b[39mraise\u001b[39;00m ReadTimeoutError(\n\u001b[0;32m    341\u001b[0m         \u001b[39mself\u001b[39m, url, \u001b[39m\"\u001b[39m\u001b[39mRead timed out. (read timeout=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m timeout_value\n\u001b[0;32m    342\u001b[0m     )\n\u001b[0;32m    344\u001b[0m \u001b[39m# See the above comment about EAGAIN in Python 3. In Python 2 we have\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[39m# to specifically catch it and throw the timeout error\u001b[39;00m\n",
      "\u001b[1;31mReadTimeoutError\u001b[0m: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mReadTimeout\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\jaden\\OneDrive\\Coding\\Program\\Python 3.11\\Lib\\site-packages\\openai\\api_requestor.py:516\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[1;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 516\u001b[0m     result \u001b[39m=\u001b[39m _thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    517\u001b[0m         method,\n\u001b[0;32m    518\u001b[0m         abs_url,\n\u001b[0;32m    519\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    520\u001b[0m         data\u001b[39m=\u001b[39;49mdata,\n\u001b[0;32m    521\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[0;32m    522\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    523\u001b[0m         timeout\u001b[39m=\u001b[39;49mrequest_timeout \u001b[39mif\u001b[39;49;00m request_timeout \u001b[39melse\u001b[39;49;00m TIMEOUT_SECS,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[0;32m    525\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mTimeout \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\jaden\\OneDrive\\Coding\\Program\\Python 3.11\\Lib\\site-packages\\requests\\sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[0;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\jaden\\OneDrive\\Coding\\Program\\Python 3.11\\Lib\\site-packages\\requests\\sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jaden\\OneDrive\\Coding\\Program\\Python 3.11\\Lib\\site-packages\\requests\\adapters.py:578\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, ReadTimeoutError):\n\u001b[1;32m--> 578\u001b[0m     \u001b[39mraise\u001b[39;00m ReadTimeout(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[0;32m    579\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, _InvalidHeader):\n",
      "\u001b[1;31mReadTimeout\u001b[0m: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTimeout\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Generate summaries of the extracted text\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m summaries \u001b[39m=\u001b[39m {title: acg\u001b[39m.\u001b[39;49msummarize_text(text) \u001b[39mfor\u001b[39;49;00m title, text \u001b[39min\u001b[39;49;00m pdf_texts\u001b[39m.\u001b[39;49mitems()}\n",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Generate summaries of the extracted text\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m summaries \u001b[39m=\u001b[39m {title: acg\u001b[39m.\u001b[39;49msummarize_text(text) \u001b[39mfor\u001b[39;00m title, text \u001b[39min\u001b[39;00m pdf_texts\u001b[39m.\u001b[39mitems()}\n",
      "Cell \u001b[1;32mIn[5], line 280\u001b[0m, in \u001b[0;36mAutomatedContentGenerator.summarize_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39m# Summarize the aggregated summary\u001b[39;00m\n\u001b[0;32m    279\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe following text are aggregated summaries of a conference. Aggregate the text and produce an report article like Bloomberg article, max 2000 words with core keywords:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00maggregated_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 280\u001b[0m response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[0;32m    281\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtext-davinci-003\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    282\u001b[0m     prompt\u001b[39m=\u001b[39;49mprompt,\n\u001b[0;32m    283\u001b[0m     max_tokens\u001b[39m=\u001b[39;49m\u001b[39m3000\u001b[39;49m,\n\u001b[0;32m    284\u001b[0m     n\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m    285\u001b[0m     stop\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    286\u001b[0m     temperature\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m,\n\u001b[0;32m    287\u001b[0m )\n\u001b[0;32m    288\u001b[0m final_summary \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mstrip()\n\u001b[0;32m    290\u001b[0m \u001b[39mreturn\u001b[39;00m final_summary\n",
      "File \u001b[1;32mc:\\Users\\jaden\\OneDrive\\Coding\\Program\\Python 3.11\\Lib\\site-packages\\openai\\api_resources\\completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[1;32mc:\\Users\\jaden\\OneDrive\\Coding\\Program\\Python 3.11\\Lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    155\u001b[0m         url,\n\u001b[0;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32mc:\\Users\\jaden\\OneDrive\\Coding\\Program\\Python 3.11\\Lib\\site-packages\\openai\\api_requestor.py:216\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    206\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    207\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    214\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    215\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m--> 216\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_raw(\n\u001b[0;32m    217\u001b[0m         method\u001b[39m.\u001b[39;49mlower(),\n\u001b[0;32m    218\u001b[0m         url,\n\u001b[0;32m    219\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    220\u001b[0m         supplied_headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    221\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[0;32m    222\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    223\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    224\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    225\u001b[0m     )\n\u001b[0;32m    226\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response(result, stream)\n\u001b[0;32m    227\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[1;32mc:\\Users\\jaden\\OneDrive\\Coding\\Program\\Python 3.11\\Lib\\site-packages\\openai\\api_requestor.py:526\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[1;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    516\u001b[0m     result \u001b[39m=\u001b[39m _thread_context\u001b[39m.\u001b[39msession\u001b[39m.\u001b[39mrequest(\n\u001b[0;32m    517\u001b[0m         method,\n\u001b[0;32m    518\u001b[0m         abs_url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         timeout\u001b[39m=\u001b[39mrequest_timeout \u001b[39mif\u001b[39;00m request_timeout \u001b[39melse\u001b[39;00m TIMEOUT_SECS,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[0;32m    525\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mTimeout \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 526\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mTimeout(\u001b[39m\"\u001b[39m\u001b[39mRequest timed out: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(e)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m    527\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mRequestException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    528\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mAPIConnectionError(\n\u001b[0;32m    529\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mError communicating with OpenAI: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(e)\n\u001b[0;32m    530\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[1;31mTimeout\u001b[0m: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)"
     ]
    }
   ],
   "source": [
    "# Generate summaries of the extracted text\n",
    "summaries = {title: acg.summarize_text(text) for title, text in pdf_texts.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the summaries to a PDF file\n",
    "acg.save_summaries(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "from pytube import YouTube\n",
    "from moviepy.editor import AudioFileClip\n",
    "import whisper\n",
    "\n",
    "class ConferenceSummarizer:\n",
    "    def __init__(self, api_key, cx, query, num_results, openai_api_key):\n",
    "        self.api_key = api_key\n",
    "        self.cx = cx\n",
    "        self.query = query\n",
    "        self.num_results = num_results\n",
    "        self.results = []\n",
    "        openai.api_key = openai_api_key  # replace with your OpenAI API key\n",
    "        \n",
    "    def download_youtube_audio(self, youtube_video_url):\n",
    "        self.youtube_video = YouTube(youtube_video_url)\n",
    "        streams = self.youtube_video.streams.filter(only_audio=True)\n",
    "        stream = streams.first()\n",
    "        stream.download(filename=f'{self.youtube_video.title}.mp4')\n",
    "        return f'{self.youtube_video.title}.mp4'\n",
    "    \n",
    "    def trim_audio(self, start_time, end_time):\n",
    "        # Load the audio file with moviepy\n",
    "        audio_clip = AudioFileClip(f'{self.youtube_video.title}.mp4')\n",
    "\n",
    "        # Trim the audio\n",
    "        trimmed_clip = audio_clip.subclip(start_time, end_time)\n",
    "\n",
    "        # Write the result to a file\n",
    "        trimmed_clip.write_audiofile(f'trimmed_{self.youtube_video.title}.mp3')\n",
    "    \n",
    "    def transcribe_audio(self):\n",
    "        model = whisper.load_model('base')\n",
    "        output = model.transcribe(f'trimmed_{self.youtube_video.title}.mp3')\n",
    "        return output\n",
    "    \n",
    "    def summarize_transcription(self, transcription):\n",
    "        # Extract the actual transcription text from the dictionary\n",
    "        transcription_text = transcription.get('transcription', '')\n",
    "        \n",
    "        # Split the transcription into chunks of 3000 tokens each\n",
    "        tokens = transcription_text.split()\n",
    "        chunks = [' '.join(tokens[i:i + 3000]) for i in range(0, len(tokens), 3000)]\n",
    "\n",
    "        summaries = []\n",
    "        for chunk in chunks:\n",
    "            # Summarize each chunk using the OpenAI API\n",
    "            prompt = f\"Please provide a bit detailed summary of the following text with keywords max 500 words:\\n{chunk}\"\n",
    "            response = openai.Completion.create(\n",
    "                model=\"text-davinci-003\",\n",
    "                prompt=prompt,\n",
    "                max_tokens=1000,\n",
    "                n=1,\n",
    "                stop=None,\n",
    "                temperature=0.5,\n",
    "            )\n",
    "            summaries.append(response.choices[0].text.strip())\n",
    "\n",
    "        # Aggregate the summaries\n",
    "        aggregated_summary = ' '.join(summaries)\n",
    "\n",
    "        # Summarize the aggregated summary\n",
    "        prompt = f\"The following text are aggregated summaries of a conference. Aggregate the text and produce an report article like Bloomberg article, max 1000 words with core keywords:\\n{aggregated_summary}\"\n",
    "        response = openai.Completion.create(\n",
    "            model=\"text-davinci-003\",\n",
    "            prompt=prompt,\n",
    "            max_tokens=2000,\n",
    "            n=1,\n",
    "            stop=None,\n",
    "            temperature=0.5,\n",
    "        )\n",
    "        final_summary = response.choices[0].text.strip()\n",
    "\n",
    "        # Save the final summary to a text file\n",
    "        with open(f'final_summary_{self.youtube_video.title}.txt', 'w') as f:\n",
    "            f.write(final_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(AudioFileClip.write_audiofile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = ConferenceSummarizer(api_key='AIzaSyCIGuLmSrM65sXknalTE4B4x8PsCZpAZ-I', cx='d4c72f123415549d9', query='fomc conference 2023', num_results=5, openai_api_key='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FOMC Press Conference, May 3, 2023.mp4'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "youtube_video_url = 'https://www.youtube.com/watch?v=ifqyTQ0Ifrw&t=855s'\n",
    "summarizer.download_youtube_audio(youtube_video_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in trimmed_FOMC Press Conference, May 3, 2023.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Trim the audio (optional)\n",
    "start_time = 1  # replace with the start time in seconds\n",
    "end_time = 440  # replace with the end time in seconds\n",
    "summarizer.trim_audio(start_time, end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcribe the audio\n",
    "transcription = summarizer.transcribe_audio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \" Good afternoon. Before discussing today's meeting, let me comment briefly on recent developments in the banking sector. Conditions in that sector have broadly improved since early March, and the U.S. banking system is sound and resilient. We will continue to monitor conditions in the sector. We are committed to learning the right lessons from this episode, and will work to prevent events like these from happening again. As a first step in that process, last week we released Vice Chair for Supervision and Bars review of the Federal Reserve's supervision and regulation of Silicon Valley Bank. The reviews findings underscore the need to address our rules and supervisory practices to make for a stronger and more resilient banking system, and I'm confident that we will do so. From the perspective of monetary policy, our focus remains squarely on our dual mandate to promote maximum employment and stable prices for the American people. My colleagues and I understand the hardship that high inflation is causing, and we remain strongly committed to bringing inflation back down to our 2 percent goal. Price stability is the responsibility of the Federal Reserve. Without price stability, the economy does not work for anyone. In particular, without price stability, we will not achieve a sustained period of strong labor market conditions that benefit all. Today, the FOMC raised its policy interest rate by a quarter percentage point. Since early last year, we've raised interest rates by a total of 5 percentage points in order to attain a stance of monetary policy that is sufficiently restrictive to return inflation to 2 percent over time. We are also continuing to reduce our securities holdings. Looking ahead, we'll take a data dependent approach in determining the extent to which additional policy-forming may be appropriate. I will have more to say about today's monetary policy actions after briefly reviewing economic developments. The U.S. economy slowed significantly last year, with real GDP rising at a below trend pace of 0.9 percent. The pace of economic growth in the first quarter of this year continued to be modest at 1.1 percent despite a pickup in consumer spending. Activity in the housing sector remains weak, largely reflecting higher mortgage rates. Higher interest rates and slower output growth also appear to be weighing on business-fixed investment. The labor market remains very tight. Over the first three months of the year, job gains averaged 345,000 jobs per month. The unemployment rate remained very low in March at 3.5 percent. Even so, there are some signs that supply and demand in the labor market are coming back into better balance. The labor force participation rate has moved up in recent months, particularly for individuals aged 25 to 54 years. General wage growth has shown some signs of easing and job vacancies have declined so far this year. But overall, labor demands still substantially exceeds the supply of available workers. Inflation remains well above our longer-run goal of 2 percent. Over the 12-month ending in March, total PCE prices rose 4.2 percent. Following the volatile food and energy categories, core PCE prices rose 4.6 percent. Inflation has moderated somewhat since the middle of last year. Nonetheless, inflation pressures continue to run high, and the process of getting inflation back down to 2 percent has a long way to go. Despite elevated inflation, longer-term inflation expectations appear to remain well anchored, as reflected in a broad range of surveys of households, businesses, and forecasters, as well as measures from financial markets. The Fed's monetary policy actions are guided by our mandate to promote maximum employment and stable prices for the American people. My colleagues and I are acutely aware that high inflation imposes significant hardship as it erodes purchasing power, especially for those least able to meet the higher costs of essentials, like food, housing, and transportation. We are highly attentive to the risks that high inflation poses to both sides of our mandate, and we are strongly committed to returning inflation to our 2 percent objective. At today's meeting, the committee raised the target range for the federal funds rate by a quarter percentage point, bringing the target range to 5 to 5 and a quarter percent, and we're continuing to the process of significantly reducing our security's holdings. With today's action, we have raised interest rates by 5 percentage points in a little more than a year. We are seeing the effects of our policy tightening on demand in the most interest rates sensitive sectors of the economy, particularly housing and investment. It will take time, however, for the full effects of monetary restraint to be realized, especially on inflation. In addition, the economy is likely to face further headwinds from tighter credit conditions. Credit conditions had already been tightening over the past year or so in response to our policy actions and a softer economic outlook. But the strains that emerged in the banking sector in early March appear to be resulting in even tighter credit conditions for households and businesses. In turn, these tighter credit conditions are likely to weigh on economic activity, hiring, and inflation. The extent of these effects remains uncertain. In light of these uncertain headwinds, along with monetary policy restraint we've put in place, our future policy actions will depend on how events unfold. In determining the extent to which additional policy affirming may be appropriate to return inflation to 2 percent over time, the committee will take into account the cumulative tightening of monetary policy, the lags with which monetary policy affects economic activity and inflation, and economic and financial developments. We will make that determination meeting by meeting based on the totality of incoming data and their implications for the outlook for economic activity and inflation. We are prepared to do more if greater monetary policy restraint is warranted. We remain committed to bringing inflation back down to our 2 percent goal and to keep our longer term inflation expectations well anchored. Increasing inflation is likely to require a period of below trend growth and some softening of labor market conditions. Restoring price stability is essential to set the stage for achieving maximum employment and stable prices over the longer run. To conclude, we understand that our actions affect communities, families, and businesses across the country. Everything we do is in service to our public mission. We at the Fed will do everything we can to achieve our maximum employment and price\", 'segments': [{'id': 0, 'seek': 0, 'start': 0.0, 'end': 11.0, 'text': ' Good afternoon.', 'tokens': [50364, 2205, 6499, 13, 50914], 'temperature': 0.0, 'avg_logprob': -0.18471016202654159, 'compression_ratio': 1.5287958115183247, 'no_speech_prob': 0.11055946350097656}, {'id': 1, 'seek': 0, 'start': 11.0, 'end': 15.0, 'text': \" Before discussing today's meeting, let me comment briefly on recent developments in\", 'tokens': [50914, 4546, 10850, 965, 311, 3440, 11, 718, 385, 2871, 10515, 322, 5162, 20862, 294, 51114], 'temperature': 0.0, 'avg_logprob': -0.18471016202654159, 'compression_ratio': 1.5287958115183247, 'no_speech_prob': 0.11055946350097656}, {'id': 2, 'seek': 0, 'start': 15.0, 'end': 17.72, 'text': ' the banking sector.', 'tokens': [51114, 264, 18261, 6977, 13, 51250], 'temperature': 0.0, 'avg_logprob': -0.18471016202654159, 'compression_ratio': 1.5287958115183247, 'no_speech_prob': 0.11055946350097656}, {'id': 3, 'seek': 0, 'start': 17.72, 'end': 21.92, 'text': ' Conditions in that sector have broadly improved since early March, and the U.S. banking system', 'tokens': [51250, 21793, 2451, 294, 300, 6977, 362, 19511, 9689, 1670, 2440, 6129, 11, 293, 264, 624, 13, 50, 13, 18261, 1185, 51460], 'temperature': 0.0, 'avg_logprob': -0.18471016202654159, 'compression_ratio': 1.5287958115183247, 'no_speech_prob': 0.11055946350097656}, {'id': 4, 'seek': 0, 'start': 21.92, 'end': 24.96, 'text': ' is sound and resilient.', 'tokens': [51460, 307, 1626, 293, 23699, 13, 51612], 'temperature': 0.0, 'avg_logprob': -0.18471016202654159, 'compression_ratio': 1.5287958115183247, 'no_speech_prob': 0.11055946350097656}, {'id': 5, 'seek': 0, 'start': 24.96, 'end': 27.72, 'text': ' We will continue to monitor conditions in the sector.', 'tokens': [51612, 492, 486, 2354, 281, 6002, 4487, 294, 264, 6977, 13, 51750], 'temperature': 0.0, 'avg_logprob': -0.18471016202654159, 'compression_ratio': 1.5287958115183247, 'no_speech_prob': 0.11055946350097656}, {'id': 6, 'seek': 2772, 'start': 27.72, 'end': 31.84, 'text': ' We are committed to learning the right lessons from this episode, and will work to prevent', 'tokens': [50364, 492, 366, 7784, 281, 2539, 264, 558, 8820, 490, 341, 3500, 11, 293, 486, 589, 281, 4871, 50570], 'temperature': 0.0, 'avg_logprob': -0.17770583476495305, 'compression_ratio': 1.6321070234113713, 'no_speech_prob': 0.0896582081913948}, {'id': 7, 'seek': 2772, 'start': 31.84, 'end': 35.16, 'text': ' events like these from happening again.', 'tokens': [50570, 3931, 411, 613, 490, 2737, 797, 13, 50736], 'temperature': 0.0, 'avg_logprob': -0.17770583476495305, 'compression_ratio': 1.6321070234113713, 'no_speech_prob': 0.0896582081913948}, {'id': 8, 'seek': 2772, 'start': 35.16, 'end': 39.22, 'text': ' As a first step in that process, last week we released Vice Chair for Supervision and', 'tokens': [50736, 1018, 257, 700, 1823, 294, 300, 1399, 11, 1036, 1243, 321, 4736, 13276, 8678, 337, 4548, 6763, 293, 50939], 'temperature': 0.0, 'avg_logprob': -0.17770583476495305, 'compression_ratio': 1.6321070234113713, 'no_speech_prob': 0.0896582081913948}, {'id': 9, 'seek': 2772, 'start': 39.22, 'end': 45.56, 'text': \" Bars review of the Federal Reserve's supervision and regulation of Silicon Valley Bank.\", 'tokens': [50939, 363, 685, 3131, 295, 264, 12380, 26049, 311, 32675, 293, 15062, 295, 25351, 10666, 8915, 13, 51256], 'temperature': 0.0, 'avg_logprob': -0.17770583476495305, 'compression_ratio': 1.6321070234113713, 'no_speech_prob': 0.0896582081913948}, {'id': 10, 'seek': 2772, 'start': 45.56, 'end': 50.08, 'text': ' The reviews findings underscore the need to address our rules and supervisory practices', 'tokens': [51256, 440, 10229, 16483, 37556, 264, 643, 281, 2985, 527, 4474, 293, 34409, 827, 7525, 51482], 'temperature': 0.0, 'avg_logprob': -0.17770583476495305, 'compression_ratio': 1.6321070234113713, 'no_speech_prob': 0.0896582081913948}, {'id': 11, 'seek': 2772, 'start': 50.08, 'end': 53.96, 'text': \" to make for a stronger and more resilient banking system, and I'm confident that we will\", 'tokens': [51482, 281, 652, 337, 257, 7249, 293, 544, 23699, 18261, 1185, 11, 293, 286, 478, 6679, 300, 321, 486, 51676], 'temperature': 0.0, 'avg_logprob': -0.17770583476495305, 'compression_ratio': 1.6321070234113713, 'no_speech_prob': 0.0896582081913948}, {'id': 12, 'seek': 2772, 'start': 53.96, 'end': 56.68, 'text': ' do so.', 'tokens': [51676, 360, 370, 13, 51812], 'temperature': 0.0, 'avg_logprob': -0.17770583476495305, 'compression_ratio': 1.6321070234113713, 'no_speech_prob': 0.0896582081913948}, {'id': 13, 'seek': 5668, 'start': 56.68, 'end': 60.72, 'text': ' From the perspective of monetary policy, our focus remains squarely on our dual mandate', 'tokens': [50364, 3358, 264, 4585, 295, 26388, 3897, 11, 527, 1879, 7023, 3732, 356, 322, 527, 11848, 23885, 50566], 'temperature': 0.0, 'avg_logprob': -0.1396764119466146, 'compression_ratio': 1.5921985815602837, 'no_speech_prob': 0.07736652344465256}, {'id': 14, 'seek': 5668, 'start': 60.72, 'end': 66.4, 'text': ' to promote maximum employment and stable prices for the American people.', 'tokens': [50566, 281, 9773, 6674, 11949, 293, 8351, 7901, 337, 264, 2665, 561, 13, 50850], 'temperature': 0.0, 'avg_logprob': -0.1396764119466146, 'compression_ratio': 1.5921985815602837, 'no_speech_prob': 0.07736652344465256}, {'id': 15, 'seek': 5668, 'start': 66.4, 'end': 71.08, 'text': ' My colleagues and I understand the hardship that high inflation is causing, and we remain', 'tokens': [50850, 1222, 7734, 293, 286, 1223, 264, 24172, 300, 1090, 15860, 307, 9853, 11, 293, 321, 6222, 51084], 'temperature': 0.0, 'avg_logprob': -0.1396764119466146, 'compression_ratio': 1.5921985815602837, 'no_speech_prob': 0.07736652344465256}, {'id': 16, 'seek': 5668, 'start': 71.08, 'end': 77.44, 'text': ' strongly committed to bringing inflation back down to our 2 percent goal.', 'tokens': [51084, 10613, 7784, 281, 5062, 15860, 646, 760, 281, 527, 568, 3043, 3387, 13, 51402], 'temperature': 0.0, 'avg_logprob': -0.1396764119466146, 'compression_ratio': 1.5921985815602837, 'no_speech_prob': 0.07736652344465256}, {'id': 17, 'seek': 5668, 'start': 77.44, 'end': 80.64, 'text': ' Price stability is the responsibility of the Federal Reserve.', 'tokens': [51402, 25803, 11826, 307, 264, 6357, 295, 264, 12380, 26049, 13, 51562], 'temperature': 0.0, 'avg_logprob': -0.1396764119466146, 'compression_ratio': 1.5921985815602837, 'no_speech_prob': 0.07736652344465256}, {'id': 18, 'seek': 5668, 'start': 80.64, 'end': 84.08, 'text': ' Without price stability, the economy does not work for anyone.', 'tokens': [51562, 9129, 3218, 11826, 11, 264, 5010, 775, 406, 589, 337, 2878, 13, 51734], 'temperature': 0.0, 'avg_logprob': -0.1396764119466146, 'compression_ratio': 1.5921985815602837, 'no_speech_prob': 0.07736652344465256}, {'id': 19, 'seek': 8408, 'start': 84.48, 'end': 89.08, 'text': ' In particular, without price stability, we will not achieve a sustained period of strong', 'tokens': [50384, 682, 1729, 11, 1553, 3218, 11826, 11, 321, 486, 406, 4584, 257, 23389, 2896, 295, 2068, 50614], 'temperature': 0.0, 'avg_logprob': -0.1801725613173618, 'compression_ratio': 1.6328125, 'no_speech_prob': 0.10727744549512863}, {'id': 20, 'seek': 8408, 'start': 89.08, 'end': 93.28, 'text': ' labor market conditions that benefit all.', 'tokens': [50614, 5938, 2142, 4487, 300, 5121, 439, 13, 50824], 'temperature': 0.0, 'avg_logprob': -0.1801725613173618, 'compression_ratio': 1.6328125, 'no_speech_prob': 0.10727744549512863}, {'id': 21, 'seek': 8408, 'start': 93.28, 'end': 98.28, 'text': ' Today, the FOMC raised its policy interest rate by a quarter percentage point.', 'tokens': [50824, 2692, 11, 264, 479, 5251, 34, 6005, 1080, 3897, 1179, 3314, 538, 257, 6555, 9668, 935, 13, 51074], 'temperature': 0.0, 'avg_logprob': -0.1801725613173618, 'compression_ratio': 1.6328125, 'no_speech_prob': 0.10727744549512863}, {'id': 22, 'seek': 8408, 'start': 98.28, 'end': 102.28, 'text': \" Since early last year, we've raised interest rates by a total of 5 percentage points in\", 'tokens': [51074, 4162, 2440, 1036, 1064, 11, 321, 600, 6005, 1179, 6846, 538, 257, 3217, 295, 1025, 9668, 2793, 294, 51274], 'temperature': 0.0, 'avg_logprob': -0.1801725613173618, 'compression_ratio': 1.6328125, 'no_speech_prob': 0.10727744549512863}, {'id': 23, 'seek': 8408, 'start': 102.28, 'end': 106.72, 'text': ' order to attain a stance of monetary policy that is sufficiently restrictive to return', 'tokens': [51274, 1668, 281, 23766, 257, 21033, 295, 26388, 3897, 300, 307, 31868, 43220, 281, 2736, 51496], 'temperature': 0.0, 'avg_logprob': -0.1801725613173618, 'compression_ratio': 1.6328125, 'no_speech_prob': 0.10727744549512863}, {'id': 24, 'seek': 8408, 'start': 106.72, 'end': 110.64, 'text': ' inflation to 2 percent over time.', 'tokens': [51496, 15860, 281, 568, 3043, 670, 565, 13, 51692], 'temperature': 0.0, 'avg_logprob': -0.1801725613173618, 'compression_ratio': 1.6328125, 'no_speech_prob': 0.10727744549512863}, {'id': 25, 'seek': 11064, 'start': 110.64, 'end': 114.88, 'text': ' We are also continuing to reduce our securities holdings.', 'tokens': [50364, 492, 366, 611, 9289, 281, 5407, 527, 38597, 1797, 1109, 13, 50576], 'temperature': 0.0, 'avg_logprob': -0.1691559640984786, 'compression_ratio': 1.4963235294117647, 'no_speech_prob': 0.1016969382762909}, {'id': 26, 'seek': 11064, 'start': 114.88, 'end': 118.76, 'text': \" Looking ahead, we'll take a data dependent approach in determining the extent to which\", 'tokens': [50576, 11053, 2286, 11, 321, 603, 747, 257, 1412, 12334, 3109, 294, 23751, 264, 8396, 281, 597, 50770], 'temperature': 0.0, 'avg_logprob': -0.1691559640984786, 'compression_ratio': 1.4963235294117647, 'no_speech_prob': 0.1016969382762909}, {'id': 27, 'seek': 11064, 'start': 118.76, 'end': 122.4, 'text': ' additional policy-forming may be appropriate.', 'tokens': [50770, 4497, 3897, 12, 48610, 815, 312, 6854, 13, 50952], 'temperature': 0.0, 'avg_logprob': -0.1691559640984786, 'compression_ratio': 1.4963235294117647, 'no_speech_prob': 0.1016969382762909}, {'id': 28, 'seek': 11064, 'start': 122.4, 'end': 127.4, 'text': \" I will have more to say about today's monetary policy actions after briefly reviewing economic\", 'tokens': [50952, 286, 486, 362, 544, 281, 584, 466, 965, 311, 26388, 3897, 5909, 934, 10515, 19576, 4836, 51202], 'temperature': 0.0, 'avg_logprob': -0.1691559640984786, 'compression_ratio': 1.4963235294117647, 'no_speech_prob': 0.1016969382762909}, {'id': 29, 'seek': 11064, 'start': 127.4, 'end': 130.52, 'text': ' developments.', 'tokens': [51202, 20862, 13, 51358], 'temperature': 0.0, 'avg_logprob': -0.1691559640984786, 'compression_ratio': 1.4963235294117647, 'no_speech_prob': 0.1016969382762909}, {'id': 30, 'seek': 11064, 'start': 130.52, 'end': 135.36, 'text': ' The U.S. economy slowed significantly last year, with real GDP rising at a below trend', 'tokens': [51358, 440, 624, 13, 50, 13, 5010, 32057, 10591, 1036, 1064, 11, 365, 957, 19599, 11636, 412, 257, 2507, 6028, 51600], 'temperature': 0.0, 'avg_logprob': -0.1691559640984786, 'compression_ratio': 1.4963235294117647, 'no_speech_prob': 0.1016969382762909}, {'id': 31, 'seek': 11064, 'start': 135.36, 'end': 139.24, 'text': ' pace of 0.9 percent.', 'tokens': [51600, 11638, 295, 1958, 13, 24, 3043, 13, 51794], 'temperature': 0.0, 'avg_logprob': -0.1691559640984786, 'compression_ratio': 1.4963235294117647, 'no_speech_prob': 0.1016969382762909}, {'id': 32, 'seek': 13924, 'start': 139.24, 'end': 144.56, 'text': ' The pace of economic growth in the first quarter of this year continued to be modest at 1.1 percent', 'tokens': [50364, 440, 11638, 295, 4836, 4599, 294, 264, 700, 6555, 295, 341, 1064, 7014, 281, 312, 25403, 412, 502, 13, 16, 3043, 50630], 'temperature': 0.0, 'avg_logprob': -0.1842280123607222, 'compression_ratio': 1.5574468085106383, 'no_speech_prob': 0.06482889503240585}, {'id': 33, 'seek': 13924, 'start': 144.56, 'end': 148.12, 'text': ' despite a pickup in consumer spending.', 'tokens': [50630, 7228, 257, 25328, 294, 9711, 6434, 13, 50808], 'temperature': 0.0, 'avg_logprob': -0.1842280123607222, 'compression_ratio': 1.5574468085106383, 'no_speech_prob': 0.06482889503240585}, {'id': 34, 'seek': 13924, 'start': 148.12, 'end': 154.24, 'text': ' Activity in the housing sector remains weak, largely reflecting higher mortgage rates.', 'tokens': [50808, 3251, 4253, 294, 264, 6849, 6977, 7023, 5336, 11, 11611, 23543, 2946, 20236, 6846, 13, 51114], 'temperature': 0.0, 'avg_logprob': -0.1842280123607222, 'compression_ratio': 1.5574468085106383, 'no_speech_prob': 0.06482889503240585}, {'id': 35, 'seek': 13924, 'start': 154.24, 'end': 158.52, 'text': ' Higher interest rates and slower output growth also appear to be weighing on business-fixed', 'tokens': [51114, 31997, 1179, 6846, 293, 14009, 5598, 4599, 611, 4204, 281, 312, 31986, 322, 1606, 12, 69, 40303, 51328], 'temperature': 0.0, 'avg_logprob': -0.1842280123607222, 'compression_ratio': 1.5574468085106383, 'no_speech_prob': 0.06482889503240585}, {'id': 36, 'seek': 13924, 'start': 158.52, 'end': 161.44, 'text': ' investment.', 'tokens': [51328, 6078, 13, 51474], 'temperature': 0.0, 'avg_logprob': -0.1842280123607222, 'compression_ratio': 1.5574468085106383, 'no_speech_prob': 0.06482889503240585}, {'id': 37, 'seek': 13924, 'start': 161.44, 'end': 163.72, 'text': ' The labor market remains very tight.', 'tokens': [51474, 440, 5938, 2142, 7023, 588, 4524, 13, 51588], 'temperature': 0.0, 'avg_logprob': -0.1842280123607222, 'compression_ratio': 1.5574468085106383, 'no_speech_prob': 0.06482889503240585}, {'id': 38, 'seek': 16372, 'start': 163.72, 'end': 169.96, 'text': ' Over the first three months of the year, job gains averaged 345,000 jobs per month.', 'tokens': [50364, 4886, 264, 700, 1045, 2493, 295, 264, 1064, 11, 1691, 16823, 18247, 2980, 805, 8465, 11, 1360, 4782, 680, 1618, 13, 50676], 'temperature': 0.0, 'avg_logprob': -0.14715002930682639, 'compression_ratio': 1.5327868852459017, 'no_speech_prob': 0.48616600036621094}, {'id': 39, 'seek': 16372, 'start': 169.96, 'end': 174.92, 'text': ' The unemployment rate remained very low in March at 3.5 percent.', 'tokens': [50676, 440, 17438, 3314, 12780, 588, 2295, 294, 6129, 412, 805, 13, 20, 3043, 13, 50924], 'temperature': 0.0, 'avg_logprob': -0.14715002930682639, 'compression_ratio': 1.5327868852459017, 'no_speech_prob': 0.48616600036621094}, {'id': 40, 'seek': 16372, 'start': 174.92, 'end': 179.04, 'text': ' Even so, there are some signs that supply and demand in the labor market are coming back', 'tokens': [50924, 2754, 370, 11, 456, 366, 512, 7880, 300, 5847, 293, 4733, 294, 264, 5938, 2142, 366, 1348, 646, 51130], 'temperature': 0.0, 'avg_logprob': -0.14715002930682639, 'compression_ratio': 1.5327868852459017, 'no_speech_prob': 0.48616600036621094}, {'id': 41, 'seek': 16372, 'start': 179.04, 'end': 181.8, 'text': ' into better balance.', 'tokens': [51130, 666, 1101, 4772, 13, 51268], 'temperature': 0.0, 'avg_logprob': -0.14715002930682639, 'compression_ratio': 1.5327868852459017, 'no_speech_prob': 0.48616600036621094}, {'id': 42, 'seek': 16372, 'start': 181.8, 'end': 186.16, 'text': ' The labor force participation rate has moved up in recent months, particularly for individuals', 'tokens': [51268, 440, 5938, 3464, 13487, 3314, 575, 4259, 493, 294, 5162, 2493, 11, 4098, 337, 5346, 51486], 'temperature': 0.0, 'avg_logprob': -0.14715002930682639, 'compression_ratio': 1.5327868852459017, 'no_speech_prob': 0.48616600036621094}, {'id': 43, 'seek': 16372, 'start': 186.16, 'end': 189.92, 'text': ' aged 25 to 54 years.', 'tokens': [51486, 21213, 3552, 281, 20793, 924, 13, 51674], 'temperature': 0.0, 'avg_logprob': -0.14715002930682639, 'compression_ratio': 1.5327868852459017, 'no_speech_prob': 0.48616600036621094}, {'id': 44, 'seek': 18992, 'start': 189.92, 'end': 194.39999999999998, 'text': ' General wage growth has shown some signs of easing and job vacancies have declined so', 'tokens': [50364, 6996, 15444, 4599, 575, 4898, 512, 7880, 295, 1195, 278, 293, 1691, 2842, 32286, 362, 29213, 370, 50588], 'temperature': 0.0, 'avg_logprob': -0.23488807678222656, 'compression_ratio': 1.4330357142857142, 'no_speech_prob': 0.3830513060092926}, {'id': 45, 'seek': 18992, 'start': 194.39999999999998, 'end': 196.35999999999999, 'text': ' far this year.', 'tokens': [50588, 1400, 341, 1064, 13, 50686], 'temperature': 0.0, 'avg_logprob': -0.23488807678222656, 'compression_ratio': 1.4330357142857142, 'no_speech_prob': 0.3830513060092926}, {'id': 46, 'seek': 18992, 'start': 196.35999999999999, 'end': 205.16, 'text': ' But overall, labor demands still substantially exceeds the supply of available workers.', 'tokens': [50686, 583, 4787, 11, 5938, 15107, 920, 30797, 43305, 264, 5847, 295, 2435, 5600, 13, 51126], 'temperature': 0.0, 'avg_logprob': -0.23488807678222656, 'compression_ratio': 1.4330357142857142, 'no_speech_prob': 0.3830513060092926}, {'id': 47, 'seek': 18992, 'start': 205.16, 'end': 209.23999999999998, 'text': ' Inflation remains well above our longer-run goal of 2 percent.', 'tokens': [51126, 682, 3423, 399, 7023, 731, 3673, 527, 2854, 12, 12997, 3387, 295, 568, 3043, 13, 51330], 'temperature': 0.0, 'avg_logprob': -0.23488807678222656, 'compression_ratio': 1.4330357142857142, 'no_speech_prob': 0.3830513060092926}, {'id': 48, 'seek': 18992, 'start': 209.23999999999998, 'end': 214.79999999999998, 'text': ' Over the 12-month ending in March, total PCE prices rose 4.2 percent.', 'tokens': [51330, 4886, 264, 2272, 12, 23534, 8121, 294, 6129, 11, 3217, 6465, 36, 7901, 10895, 1017, 13, 17, 3043, 13, 51608], 'temperature': 0.0, 'avg_logprob': -0.23488807678222656, 'compression_ratio': 1.4330357142857142, 'no_speech_prob': 0.3830513060092926}, {'id': 49, 'seek': 21480, 'start': 214.8, 'end': 221.08, 'text': ' Following the volatile food and energy categories, core PCE prices rose 4.6 percent.', 'tokens': [50364, 19192, 264, 34377, 1755, 293, 2281, 10479, 11, 4965, 6465, 36, 7901, 10895, 1017, 13, 21, 3043, 13, 50678], 'temperature': 0.0, 'avg_logprob': -0.15014884115635663, 'compression_ratio': 1.5833333333333333, 'no_speech_prob': 0.6278590559959412}, {'id': 50, 'seek': 21480, 'start': 221.08, 'end': 224.44, 'text': ' Inflation has moderated somewhat since the middle of last year.', 'tokens': [50678, 682, 3423, 399, 575, 10494, 770, 8344, 1670, 264, 2808, 295, 1036, 1064, 13, 50846], 'temperature': 0.0, 'avg_logprob': -0.15014884115635663, 'compression_ratio': 1.5833333333333333, 'no_speech_prob': 0.6278590559959412}, {'id': 51, 'seek': 21480, 'start': 224.44, 'end': 229.44, 'text': ' Nonetheless, inflation pressures continue to run high, and the process of getting inflation', 'tokens': [50846, 45437, 11, 15860, 23573, 2354, 281, 1190, 1090, 11, 293, 264, 1399, 295, 1242, 15860, 51096], 'temperature': 0.0, 'avg_logprob': -0.15014884115635663, 'compression_ratio': 1.5833333333333333, 'no_speech_prob': 0.6278590559959412}, {'id': 52, 'seek': 21480, 'start': 229.44, 'end': 233.72000000000003, 'text': ' back down to 2 percent has a long way to go.', 'tokens': [51096, 646, 760, 281, 568, 3043, 575, 257, 938, 636, 281, 352, 13, 51310], 'temperature': 0.0, 'avg_logprob': -0.15014884115635663, 'compression_ratio': 1.5833333333333333, 'no_speech_prob': 0.6278590559959412}, {'id': 53, 'seek': 21480, 'start': 233.72000000000003, 'end': 238.92000000000002, 'text': ' Despite elevated inflation, longer-term inflation expectations appear to remain well anchored,', 'tokens': [51310, 11334, 23457, 15860, 11, 2854, 12, 7039, 15860, 9843, 4204, 281, 6222, 731, 12723, 2769, 11, 51570], 'temperature': 0.0, 'avg_logprob': -0.15014884115635663, 'compression_ratio': 1.5833333333333333, 'no_speech_prob': 0.6278590559959412}, {'id': 54, 'seek': 23892, 'start': 238.92, 'end': 244.6, 'text': ' as reflected in a broad range of surveys of households, businesses, and forecasters,', 'tokens': [50364, 382, 15502, 294, 257, 4152, 3613, 295, 22711, 295, 22850, 11, 6011, 11, 293, 2091, 16369, 1559, 11, 50648], 'temperature': 0.0, 'avg_logprob': -0.1304673453172048, 'compression_ratio': 1.5571428571428572, 'no_speech_prob': 0.4944823682308197}, {'id': 55, 'seek': 23892, 'start': 244.6, 'end': 249.39999999999998, 'text': ' as well as measures from financial markets.', 'tokens': [50648, 382, 731, 382, 8000, 490, 4669, 8383, 13, 50888], 'temperature': 0.0, 'avg_logprob': -0.1304673453172048, 'compression_ratio': 1.5571428571428572, 'no_speech_prob': 0.4944823682308197}, {'id': 56, 'seek': 23892, 'start': 249.39999999999998, 'end': 254.83999999999997, 'text': \" The Fed's monetary policy actions are guided by our mandate to promote maximum employment\", 'tokens': [50888, 440, 7772, 311, 26388, 3897, 5909, 366, 19663, 538, 527, 23885, 281, 9773, 6674, 11949, 51160], 'temperature': 0.0, 'avg_logprob': -0.1304673453172048, 'compression_ratio': 1.5571428571428572, 'no_speech_prob': 0.4944823682308197}, {'id': 57, 'seek': 23892, 'start': 254.83999999999997, 'end': 258.12, 'text': ' and stable prices for the American people.', 'tokens': [51160, 293, 8351, 7901, 337, 264, 2665, 561, 13, 51324], 'temperature': 0.0, 'avg_logprob': -0.1304673453172048, 'compression_ratio': 1.5571428571428572, 'no_speech_prob': 0.4944823682308197}, {'id': 58, 'seek': 23892, 'start': 258.12, 'end': 263.24, 'text': ' My colleagues and I are acutely aware that high inflation imposes significant hardship', 'tokens': [51324, 1222, 7734, 293, 286, 366, 696, 325, 736, 3650, 300, 1090, 15860, 704, 4201, 4776, 24172, 51580], 'temperature': 0.0, 'avg_logprob': -0.1304673453172048, 'compression_ratio': 1.5571428571428572, 'no_speech_prob': 0.4944823682308197}, {'id': 59, 'seek': 23892, 'start': 263.24, 'end': 268.03999999999996, 'text': ' as it erodes purchasing power, especially for those least able to meet the higher costs', 'tokens': [51580, 382, 309, 1189, 4789, 20906, 1347, 11, 2318, 337, 729, 1935, 1075, 281, 1677, 264, 2946, 5497, 51820], 'temperature': 0.0, 'avg_logprob': -0.1304673453172048, 'compression_ratio': 1.5571428571428572, 'no_speech_prob': 0.4944823682308197}, {'id': 60, 'seek': 26804, 'start': 268.04, 'end': 272.68, 'text': ' of essentials, like food, housing, and transportation.', 'tokens': [50364, 295, 46884, 11, 411, 1755, 11, 6849, 11, 293, 11328, 13, 50596], 'temperature': 0.0, 'avg_logprob': -0.15355359183417427, 'compression_ratio': 1.7076271186440677, 'no_speech_prob': 0.21664315462112427}, {'id': 61, 'seek': 26804, 'start': 272.68, 'end': 277.96000000000004, 'text': ' We are highly attentive to the risks that high inflation poses to both sides of our mandate,', 'tokens': [50596, 492, 366, 5405, 43661, 281, 264, 10888, 300, 1090, 15860, 26059, 281, 1293, 4881, 295, 527, 23885, 11, 50860], 'temperature': 0.0, 'avg_logprob': -0.15355359183417427, 'compression_ratio': 1.7076271186440677, 'no_speech_prob': 0.21664315462112427}, {'id': 62, 'seek': 26804, 'start': 277.96000000000004, 'end': 284.48, 'text': ' and we are strongly committed to returning inflation to our 2 percent objective.', 'tokens': [50860, 293, 321, 366, 10613, 7784, 281, 12678, 15860, 281, 527, 568, 3043, 10024, 13, 51186], 'temperature': 0.0, 'avg_logprob': -0.15355359183417427, 'compression_ratio': 1.7076271186440677, 'no_speech_prob': 0.21664315462112427}, {'id': 63, 'seek': 26804, 'start': 284.48, 'end': 287.68, 'text': \" At today's meeting, the committee raised the target range for the federal funds rate\", 'tokens': [51186, 1711, 965, 311, 3440, 11, 264, 7482, 6005, 264, 3779, 3613, 337, 264, 6019, 8271, 3314, 51346], 'temperature': 0.0, 'avg_logprob': -0.15355359183417427, 'compression_ratio': 1.7076271186440677, 'no_speech_prob': 0.21664315462112427}, {'id': 64, 'seek': 26804, 'start': 287.68, 'end': 292.92, 'text': ' by a quarter percentage point, bringing the target range to 5 to 5 and a quarter percent,', 'tokens': [51346, 538, 257, 6555, 9668, 935, 11, 5062, 264, 3779, 3613, 281, 1025, 281, 1025, 293, 257, 6555, 3043, 11, 51608], 'temperature': 0.0, 'avg_logprob': -0.15355359183417427, 'compression_ratio': 1.7076271186440677, 'no_speech_prob': 0.21664315462112427}, {'id': 65, 'seek': 29292, 'start': 292.92, 'end': 298.6, 'text': \" and we're continuing to the process of significantly reducing our security's holdings.\", 'tokens': [50364, 293, 321, 434, 9289, 281, 264, 1399, 295, 10591, 12245, 527, 3825, 311, 1797, 1109, 13, 50648], 'temperature': 0.0, 'avg_logprob': -0.15676498413085938, 'compression_ratio': 1.6405693950177935, 'no_speech_prob': 0.008382602594792843}, {'id': 66, 'seek': 29292, 'start': 298.6, 'end': 302.0, 'text': \" With today's action, we have raised interest rates by 5 percentage points in a little more\", 'tokens': [50648, 2022, 965, 311, 3069, 11, 321, 362, 6005, 1179, 6846, 538, 1025, 9668, 2793, 294, 257, 707, 544, 50818], 'temperature': 0.0, 'avg_logprob': -0.15676498413085938, 'compression_ratio': 1.6405693950177935, 'no_speech_prob': 0.008382602594792843}, {'id': 67, 'seek': 29292, 'start': 302.0, 'end': 303.52000000000004, 'text': ' than a year.', 'tokens': [50818, 813, 257, 1064, 13, 50894], 'temperature': 0.0, 'avg_logprob': -0.15676498413085938, 'compression_ratio': 1.6405693950177935, 'no_speech_prob': 0.008382602594792843}, {'id': 68, 'seek': 29292, 'start': 303.52000000000004, 'end': 307.76, 'text': ' We are seeing the effects of our policy tightening on demand in the most interest rates sensitive', 'tokens': [50894, 492, 366, 2577, 264, 5065, 295, 527, 3897, 42217, 322, 4733, 294, 264, 881, 1179, 6846, 9477, 51106], 'temperature': 0.0, 'avg_logprob': -0.15676498413085938, 'compression_ratio': 1.6405693950177935, 'no_speech_prob': 0.008382602594792843}, {'id': 69, 'seek': 29292, 'start': 307.76, 'end': 312.08000000000004, 'text': ' sectors of the economy, particularly housing and investment.', 'tokens': [51106, 18373, 295, 264, 5010, 11, 4098, 6849, 293, 6078, 13, 51322], 'temperature': 0.0, 'avg_logprob': -0.15676498413085938, 'compression_ratio': 1.6405693950177935, 'no_speech_prob': 0.008382602594792843}, {'id': 70, 'seek': 29292, 'start': 312.08000000000004, 'end': 317.20000000000005, 'text': ' It will take time, however, for the full effects of monetary restraint to be realized, especially', 'tokens': [51322, 467, 486, 747, 565, 11, 4461, 11, 337, 264, 1577, 5065, 295, 26388, 49281, 281, 312, 5334, 11, 2318, 51578], 'temperature': 0.0, 'avg_logprob': -0.15676498413085938, 'compression_ratio': 1.6405693950177935, 'no_speech_prob': 0.008382602594792843}, {'id': 71, 'seek': 29292, 'start': 317.20000000000005, 'end': 321.16, 'text': ' on inflation.', 'tokens': [51578, 322, 15860, 13, 51776], 'temperature': 0.0, 'avg_logprob': -0.15676498413085938, 'compression_ratio': 1.6405693950177935, 'no_speech_prob': 0.008382602594792843}, {'id': 72, 'seek': 32116, 'start': 321.16, 'end': 326.88000000000005, 'text': ' In addition, the economy is likely to face further headwinds from tighter credit conditions.', 'tokens': [50364, 682, 4500, 11, 264, 5010, 307, 3700, 281, 1851, 3052, 1378, 12199, 82, 490, 30443, 5397, 4487, 13, 50650], 'temperature': 0.0, 'avg_logprob': -0.14500902249262884, 'compression_ratio': 1.8345864661654134, 'no_speech_prob': 0.2729431390762329}, {'id': 73, 'seek': 32116, 'start': 326.88000000000005, 'end': 331.24, 'text': ' Credit conditions had already been tightening over the past year or so in response to our policy', 'tokens': [50650, 36006, 4487, 632, 1217, 668, 42217, 670, 264, 1791, 1064, 420, 370, 294, 4134, 281, 527, 3897, 50868], 'temperature': 0.0, 'avg_logprob': -0.14500902249262884, 'compression_ratio': 1.8345864661654134, 'no_speech_prob': 0.2729431390762329}, {'id': 74, 'seek': 32116, 'start': 331.24, 'end': 335.32000000000005, 'text': ' actions and a softer economic outlook.', 'tokens': [50868, 5909, 293, 257, 23119, 4836, 26650, 13, 51072], 'temperature': 0.0, 'avg_logprob': -0.14500902249262884, 'compression_ratio': 1.8345864661654134, 'no_speech_prob': 0.2729431390762329}, {'id': 75, 'seek': 32116, 'start': 335.32000000000005, 'end': 339.44000000000005, 'text': ' But the strains that emerged in the banking sector in early March appear to be resulting', 'tokens': [51072, 583, 264, 39110, 300, 20178, 294, 264, 18261, 6977, 294, 2440, 6129, 4204, 281, 312, 16505, 51278], 'temperature': 0.0, 'avg_logprob': -0.14500902249262884, 'compression_ratio': 1.8345864661654134, 'no_speech_prob': 0.2729431390762329}, {'id': 76, 'seek': 32116, 'start': 339.44000000000005, 'end': 344.08000000000004, 'text': ' in even tighter credit conditions for households and businesses.', 'tokens': [51278, 294, 754, 30443, 5397, 4487, 337, 22850, 293, 6011, 13, 51510], 'temperature': 0.0, 'avg_logprob': -0.14500902249262884, 'compression_ratio': 1.8345864661654134, 'no_speech_prob': 0.2729431390762329}, {'id': 77, 'seek': 32116, 'start': 344.08000000000004, 'end': 348.48, 'text': ' In turn, these tighter credit conditions are likely to weigh on economic activity, hiring,', 'tokens': [51510, 682, 1261, 11, 613, 30443, 5397, 4487, 366, 3700, 281, 13843, 322, 4836, 5191, 11, 15335, 11, 51730], 'temperature': 0.0, 'avg_logprob': -0.14500902249262884, 'compression_ratio': 1.8345864661654134, 'no_speech_prob': 0.2729431390762329}, {'id': 78, 'seek': 32116, 'start': 348.48, 'end': 349.48, 'text': ' and inflation.', 'tokens': [51730, 293, 15860, 13, 51780], 'temperature': 0.0, 'avg_logprob': -0.14500902249262884, 'compression_ratio': 1.8345864661654134, 'no_speech_prob': 0.2729431390762329}, {'id': 79, 'seek': 34948, 'start': 350.12, 'end': 354.52000000000004, 'text': ' The extent of these effects remains uncertain.', 'tokens': [50396, 440, 8396, 295, 613, 5065, 7023, 11308, 13, 50616], 'temperature': 0.0, 'avg_logprob': -0.14604791320196472, 'compression_ratio': 1.7941176470588236, 'no_speech_prob': 0.21803225576877594}, {'id': 80, 'seek': 34948, 'start': 354.52000000000004, 'end': 359.44, 'text': \" In light of these uncertain headwinds, along with monetary policy restraint we've put in place,\", 'tokens': [50616, 682, 1442, 295, 613, 11308, 1378, 12199, 82, 11, 2051, 365, 26388, 3897, 49281, 321, 600, 829, 294, 1081, 11, 50862], 'temperature': 0.0, 'avg_logprob': -0.14604791320196472, 'compression_ratio': 1.7941176470588236, 'no_speech_prob': 0.21803225576877594}, {'id': 81, 'seek': 34948, 'start': 359.44, 'end': 364.48, 'text': ' our future policy actions will depend on how events unfold.', 'tokens': [50862, 527, 2027, 3897, 5909, 486, 5672, 322, 577, 3931, 17980, 13, 51114], 'temperature': 0.0, 'avg_logprob': -0.14604791320196472, 'compression_ratio': 1.7941176470588236, 'no_speech_prob': 0.21803225576877594}, {'id': 82, 'seek': 34948, 'start': 364.48, 'end': 368.84000000000003, 'text': ' In determining the extent to which additional policy affirming may be appropriate to return', 'tokens': [51114, 682, 23751, 264, 8396, 281, 597, 4497, 3897, 36315, 2810, 815, 312, 6854, 281, 2736, 51332], 'temperature': 0.0, 'avg_logprob': -0.14604791320196472, 'compression_ratio': 1.7941176470588236, 'no_speech_prob': 0.21803225576877594}, {'id': 83, 'seek': 34948, 'start': 368.84000000000003, 'end': 373.92, 'text': ' inflation to 2 percent over time, the committee will take into account the cumulative tightening', 'tokens': [51332, 15860, 281, 568, 3043, 670, 565, 11, 264, 7482, 486, 747, 666, 2696, 264, 38379, 42217, 51586], 'temperature': 0.0, 'avg_logprob': -0.14604791320196472, 'compression_ratio': 1.7941176470588236, 'no_speech_prob': 0.21803225576877594}, {'id': 84, 'seek': 34948, 'start': 373.92, 'end': 379.44, 'text': ' of monetary policy, the lags with which monetary policy affects economic activity and inflation,', 'tokens': [51586, 295, 26388, 3897, 11, 264, 8953, 82, 365, 597, 26388, 3897, 11807, 4836, 5191, 293, 15860, 11, 51862], 'temperature': 0.0, 'avg_logprob': -0.14604791320196472, 'compression_ratio': 1.7941176470588236, 'no_speech_prob': 0.21803225576877594}, {'id': 85, 'seek': 37944, 'start': 380.24, 'end': 383.44, 'text': ' and economic and financial developments.', 'tokens': [50404, 293, 4836, 293, 4669, 20862, 13, 50564], 'temperature': 0.0, 'avg_logprob': -0.1781222386793657, 'compression_ratio': 1.7448559670781894, 'no_speech_prob': 0.009725332260131836}, {'id': 86, 'seek': 37944, 'start': 383.44, 'end': 389.04, 'text': ' We will make that determination meeting by meeting based on the totality of incoming data', 'tokens': [50564, 492, 486, 652, 300, 18432, 3440, 538, 3440, 2361, 322, 264, 1993, 1860, 295, 22341, 1412, 50844], 'temperature': 0.0, 'avg_logprob': -0.1781222386793657, 'compression_ratio': 1.7448559670781894, 'no_speech_prob': 0.009725332260131836}, {'id': 87, 'seek': 37944, 'start': 389.04, 'end': 393.6, 'text': ' and their implications for the outlook for economic activity and inflation.', 'tokens': [50844, 293, 641, 16602, 337, 264, 26650, 337, 4836, 5191, 293, 15860, 13, 51072], 'temperature': 0.0, 'avg_logprob': -0.1781222386793657, 'compression_ratio': 1.7448559670781894, 'no_speech_prob': 0.009725332260131836}, {'id': 88, 'seek': 37944, 'start': 393.6, 'end': 399.72, 'text': ' We are prepared to do more if greater monetary policy restraint is warranted.', 'tokens': [51072, 492, 366, 4927, 281, 360, 544, 498, 5044, 26388, 3897, 49281, 307, 16354, 292, 13, 51378], 'temperature': 0.0, 'avg_logprob': -0.1781222386793657, 'compression_ratio': 1.7448559670781894, 'no_speech_prob': 0.009725332260131836}, {'id': 89, 'seek': 37944, 'start': 399.72, 'end': 404.08, 'text': ' We remain committed to bringing inflation back down to our 2 percent goal and to keep', 'tokens': [51378, 492, 6222, 7784, 281, 5062, 15860, 646, 760, 281, 527, 568, 3043, 3387, 293, 281, 1066, 51596], 'temperature': 0.0, 'avg_logprob': -0.1781222386793657, 'compression_ratio': 1.7448559670781894, 'no_speech_prob': 0.009725332260131836}, {'id': 90, 'seek': 37944, 'start': 404.08, 'end': 408.84, 'text': ' our longer term inflation expectations well anchored.', 'tokens': [51596, 527, 2854, 1433, 15860, 9843, 731, 12723, 2769, 13, 51834], 'temperature': 0.0, 'avg_logprob': -0.1781222386793657, 'compression_ratio': 1.7448559670781894, 'no_speech_prob': 0.009725332260131836}, {'id': 91, 'seek': 40884, 'start': 408.84, 'end': 413.64, 'text': ' Increasing inflation is likely to require a period of below trend growth and some softening', 'tokens': [50364, 30367, 3349, 15860, 307, 3700, 281, 3651, 257, 2896, 295, 2507, 6028, 4599, 293, 512, 2787, 4559, 50604], 'temperature': 0.0, 'avg_logprob': -0.19933948516845704, 'compression_ratio': 1.5891472868217054, 'no_speech_prob': 0.046327222138643265}, {'id': 92, 'seek': 40884, 'start': 413.64, 'end': 415.96, 'text': ' of labor market conditions.', 'tokens': [50604, 295, 5938, 2142, 4487, 13, 50720], 'temperature': 0.0, 'avg_logprob': -0.19933948516845704, 'compression_ratio': 1.5891472868217054, 'no_speech_prob': 0.046327222138643265}, {'id': 93, 'seek': 40884, 'start': 415.96, 'end': 421.23999999999995, 'text': ' Restoring price stability is essential to set the stage for achieving maximum employment', 'tokens': [50720, 13094, 3662, 3218, 11826, 307, 7115, 281, 992, 264, 3233, 337, 19626, 6674, 11949, 50984], 'temperature': 0.0, 'avg_logprob': -0.19933948516845704, 'compression_ratio': 1.5891472868217054, 'no_speech_prob': 0.046327222138643265}, {'id': 94, 'seek': 40884, 'start': 421.23999999999995, 'end': 424.91999999999996, 'text': ' and stable prices over the longer run.', 'tokens': [50984, 293, 8351, 7901, 670, 264, 2854, 1190, 13, 51168], 'temperature': 0.0, 'avg_logprob': -0.19933948516845704, 'compression_ratio': 1.5891472868217054, 'no_speech_prob': 0.046327222138643265}, {'id': 95, 'seek': 40884, 'start': 424.91999999999996, 'end': 429.79999999999995, 'text': ' To conclude, we understand that our actions affect communities, families, and businesses', 'tokens': [51168, 1407, 16886, 11, 321, 1223, 300, 527, 5909, 3345, 4456, 11, 4466, 11, 293, 6011, 51412], 'temperature': 0.0, 'avg_logprob': -0.19933948516845704, 'compression_ratio': 1.5891472868217054, 'no_speech_prob': 0.046327222138643265}, {'id': 96, 'seek': 40884, 'start': 429.79999999999995, 'end': 431.84, 'text': ' across the country.', 'tokens': [51412, 2108, 264, 1941, 13, 51514], 'temperature': 0.0, 'avg_logprob': -0.19933948516845704, 'compression_ratio': 1.5891472868217054, 'no_speech_prob': 0.046327222138643265}, {'id': 97, 'seek': 40884, 'start': 431.84, 'end': 435.03999999999996, 'text': ' Everything we do is in service to our public mission.', 'tokens': [51514, 5471, 321, 360, 307, 294, 2643, 281, 527, 1908, 4447, 13, 51674], 'temperature': 0.0, 'avg_logprob': -0.19933948516845704, 'compression_ratio': 1.5891472868217054, 'no_speech_prob': 0.046327222138643265}, {'id': 98, 'seek': 43504, 'start': 435.04, 'end': 438.92, 'text': ' We at the Fed will do everything we can to achieve our maximum employment and price', 'tokens': [50364, 492, 412, 264, 7772, 486, 360, 1203, 321, 393, 281, 4584, 527, 6674, 11949, 293, 3218, 50558], 'temperature': 0.0, 'avg_logprob': -0.24173779236642937, 'compression_ratio': 1.0921052631578947, 'no_speech_prob': 0.9447956085205078}], 'language': 'en'}\n"
     ]
    }
   ],
   "source": [
    "print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the transcription\n",
    "summarizer.summarize_transcription(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(transcription)\n",
    "df.to_excel('transcription.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in trimmed_FOMC Press Conference, May 3, 2023.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the ConferenceSummarizer class\n",
    "summarizer = ConferenceSummarizer(api_key='AIzaSyCIGuLmSrM65sXknalTE4B4x8PsCZpAZ-I', cx='d4c72f123415549d9', query='fomc conference 2023', num_results=5, openai_api_key='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')\n",
    "\n",
    "# Download the audio from a YouTube video\n",
    "youtube_video_url = 'https://www.youtube.com/watch?v=ifqyTQ0Ifrw&t=855s'  # replace with your YouTube video URL\n",
    "summarizer.download_youtube_audio(youtube_video_url)\n",
    "\n",
    "# Trim the audio (optional)\n",
    "start_time = 1  # replace with the start time in seconds\n",
    "end_time = 440  # replace with the end time in seconds\n",
    "summarizer.trim_audio(start_time, end_time)\n",
    "\n",
    "# Transcribe the audio\n",
    "transcription = summarizer.transcribe_audio()\n",
    "\n",
    "# Summarize the transcription\n",
    "summarizer.summarize_transcription(transcription)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
