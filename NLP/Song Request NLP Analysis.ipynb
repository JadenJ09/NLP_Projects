{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a model that can find and correct typos in text, you can use natural language processing (NLP) techniques and machine learning algorithms. Here is a general outline of the steps you could take:\n",
    "\n",
    "1. Prepare the data: Collect a dataset of text data that includes both correct and incorrect spellings. You can use a tool like the Natural Language Toolkit (NLTK) in Python to tokenize the data and extract features.\n",
    "\n",
    "2. Train the model: Use a machine learning algorithm to train a model on the prepared data. You can use algorithms like logistic regression, decision trees, or neural networks to build a model that can predict the correct spelling of a word based on its context and surrounding words.\n",
    "\n",
    "3. Save the model: Once you have trained the model, you can save it as a file, such as a CSV file or a pickle file, for future use.\n",
    "\n",
    "4. Apply the model: To use the model to correct typos, you can load the saved model and apply it to new text data. Here is some sample code that shows how to load a model from a CSV file and use it to correct typos in a sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thsi is an example of a setnence with typo mistakes .\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Load the model from a CSV file\n",
    "model_data = pd.read_csv('C:\\\\Coding\\\\Python\\\\NLP\\\\data\\\\test_data.csv')\n",
    "model = model_data.to_numpy()\n",
    "\n",
    "# Define a function to correct typos in a sentence\n",
    "def correct_sentence(sentence):\n",
    "    # Tokenize the sentence into words\n",
    "    words = word_tokenize(sentence)\n",
    "\n",
    "    # Iterate through the words in the sentence\n",
    "    for i, word in enumerate(words):\n",
    "        # Check if the word is in the model\n",
    "        if word in model[:,0]:\n",
    "            # Find the correct spelling of the word in the model\n",
    "            correct_word = model[model[:,0] == word, 1][0]\n",
    "\n",
    "            # Replace the typo with the correct spelling\n",
    "            words[i] = correct_word\n",
    "\n",
    "    # Combine the corrected words into a new sentence\n",
    "    corrected_sentence = ' '.join(words)\n",
    "\n",
    "    return corrected_sentence\n",
    "\n",
    "# Example usage\n",
    "sentence = \"Thsi is an example of a setnence with typo mistakes.\"\n",
    "corrected_sentence = correct_sentence(sentence)\n",
    "print(corrected_sentence)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the Spotify Web API to extract song data, including information about the artist, song, album, and release year, in a programmatic way. Here's an outline of the steps you can follow to extract song data from Spotify:\n",
    "\n",
    "1. Set up a Spotify developer account: To use the Spotify Web API, you need to set up a Spotify developer account and create a new application. Once you have created an application, you can get an access token to use the API.\n",
    "\n",
    "2. Make API requests: Use a Python library like spotipy to make API requests to the Spotify Web API. You can use the search endpoint to search for songs, albums, and artists based on keywords. For example, you can search for all songs by an artist, or all songs in an album. The API returns a JSON object with information about the songs, including the artist name, song name, album name, and release year.\n",
    "\n",
    "3. Store the data: Once you have extracted the data from the API, you can store it in a file format that is suitable for your project, such as a CSV file. You can use the pandas library in Python to create a DataFrame from the API data and then save the DataFrame to a CSV file.\n",
    "\n",
    "Here's some sample code that shows how to use the spotipy library to search for songs by a particular artist and extract information about the songs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spotipy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mspotipy\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mspotipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moauth2\u001b[39;00m \u001b[39mimport\u001b[39;00m SpotifyClientCredentials\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spotipy'"
     ]
    }
   ],
   "source": [
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "import pandas as pd\n",
    "\n",
    "# Set up Spotify credentials\n",
    "client_credentials_manager = SpotifyClientCredentials(client_id='5bfef1fd889641d08e029c22f6e5d899', client_secret='92dafad372dc4145b05cb13b7f29df2b')\n",
    "sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)\n",
    "\n",
    "# Search for tracks by multiple artists\n",
    "artists = ['Taylor Swift', 'Ed Sheeran', 'Adele']\n",
    "results = sp.search(q='artist:' + ','.join(artists), type='track', limit=50)\n",
    "\n",
    "# Extract information about the tracks\n",
    "tracks = []\n",
    "for track in results['tracks']['items']:\n",
    "    track_name = track['name']\n",
    "    artist_names = [artist['name'] for artist in track['artists']]\n",
    "    album_name = track['album']['name']\n",
    "    release_year = track['album']['release_date'][:4]\n",
    "    popularity = track['popularity']\n",
    "    tracks.append({'track_name': track_name, 'artist_names': artist_names, 'album_name': album_name, 'release_year': release_year, 'popularity': popularity})\n",
    "\n",
    "# Print the tracks\n",
    "for track in tracks:\n",
    "    print(track['track_name'], ' - '.join(track['artist_names']), track['album_name'], track['release_year'], track['popularity'])\n",
    "\n",
    "# Convert the data to a pandas DataFrame and save to a CSV file\n",
    "songs_df = pd.DataFrame(tracks)\n",
    "songs_df.to_csv('songs.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a12d51c86ae5a9e0728790e9f226ac055370ed14b27e8d69cccd17f51d83081"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
